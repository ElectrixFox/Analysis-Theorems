\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\newcommand{\limas}[3][n]{#2 \to #3 \text{ as } #1 \to \infty}
\newcommand{\seq}[1][x]{(#1_n)_{n \in \N}}
\newcommand{\dseq}[2][n]{(#2_#1)_{#1 \in \N}}

\title{Analysis I}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Basic logic and sets}

\begin{theorem}[Bernoulli Inequality]\label{analy:thm:bernoineq}
    \lean{bernoineq}
    Let $x \geq -1$ and $n \in \N$. Then
    \[
    (1 + x) ^ n \geq 1 + nx.
    \]
    \begin{proof}
        For $x = -1\quad 0 \geq 1 - n$ so this is true.

        So assume $x > -1$. Now we use induction on $n$.

        $n = 1$
        \[
        1 + x \geq 1 + x\text{ this is even an equality}
        \]

        Assume statement holds for some $n \in \N$. Then
        \begin{align*}
            (1 + x) ^ {n + 1} &= \overset{0 >}{(1 + x)}(1 + x) ^ n \\
            &\geq (1 + x)(1 + nx) \\
            &= 1 + nx + x + nx ^ 2 \\
            &= 1 + (n + 1)x + nx ^ 2 \\
            &\geq 1 + (n + 1)x.
        \end{align*}
        By induction, Bernoulli inequality holds.
    \end{proof}
\end{theorem}

\subsection{The Completeness Axiom}

\begin{definition}[Bounded above and beow]
    Let $X \subset \R$. We say that $X$ is bounded above, if there exists a $c \in \R$ with $z \leq c$ for all $x \in X$.
    
    Similar, $X$ is bounded below, if there is a $c \in \R$ with $c \leq x$ for all $x \in X$. Such $c$ is called a lower bound for $X$.
    
    If $X$ is bounded above and bounded below, we say $X$ is bounded.
\end{definition}

\begin{definition}[Maximum and Minimum]
    Let $X \subset \R$. An element $M \in X$ is called the maximum of $X$, if $x \leq M$ for all $x \in X$, we write $M = \max X$, if such $M$ exists.

    Similarly, $m \in X$ is called the minimum of $X$, if $m \leq x$ for all $x \in M$, we write $m = \min X$, if such $m$ exists.
\end{definition}

\begin{definition}[Supremum]
    Let $X$ be a set which is bounded above.
    A number $C \in \R$ is called the supremum of $X$,
    or the least upper bound of $X$, if
    \begin{enumerate}[label = (\alph*)]
        \item $C$ is an upper bound of $X$.
        \item Whenever $B \in \R$ is another upper bound of $X$, the $C \leq B$.
    \end{enumerate}
    We write $\sup X$ for such a $C$.
\end{definition}

\begin{definition}
    Let $X$ be a set which is bounded below.
    A number $c \in \R$ is called the infimum of $X$ or greatest lower bound of $X$, if
    \begin{enumerate}[label = (\alph*)]
        \item $c$ is an lower bound of $X$.
        \item Whenever $b \in \R$ is a lower bound of $X$, then $b \leq c$.
    \end{enumerate}
    We write $\inf X$ for such $c$.
\end{definition}

\begin{lemma}
    If $X$ has a maximum $M = \max{X}$,
    then $M$ is the supremum.

    \begin{proof}
        We have $x \leq M$ for all $x \in X$ giving the first condition.
        If $B$ is an upper bound of $X$ then $M \leq B$ since $M \in X$ then the second condition is satisfied.
    \end{proof}
\end{lemma}

\begin{definition}[Completeness Axiom]\label{ax:compl}
    Every non-empty subset of $\R$ which is bounded above has a supremum.
\end{definition}

\begin{theorem}[Archimedes]\label{analy:thm:archim}
    Let $a, b \in \R$ with $b > 0$. Then there exists an $n \in \N$ with $nb > a$.
    [Think $b = 1$, then this is saying for every $a \in \R$ there is $n \in \N$ with $n > a$.]
    \begin{proof}
        Assume not.
        Then the set $X = \{nb \in \R \,|\, n \in \N\}$ is bounded above by $a$.
        It is also non-empty:
        $b \in X$.
        By \autoref{ax:compl} we have a supremum $C = \sup X$.

        Look at $C - b < C$,
        this $C - b$ cannot be an upper bound of $X$.
        So there is $x \in X$ with $nb = x > C - b \iff nb + b = \underset{\in X}{(n + 1)b} > C$
        Contradiction $C$ being an upper bound of $X$.
    \end{proof}
\end{theorem}

\begin{theorem}
    Let $a \geq 0$ and $p \in \N$.
    Then there exists a unique $x \geq 0$ with $x ^ p = a$.
    We call $x$ the $p$-th root of $a$,
    and write $\sqrt[p]{a} = x$.
    This gives us functions $\sqrt[p]{\cdot} : [0, \infty) \to [0, \infty)$.

    For odd $p$ we can extend this to $\sqrt[p]{\cdot} : \R \to \R$.
    If $a < 0$, then let $\sqrt[p]{a} = -\sqrt[p]{-a}$.
    \begin{proof}
        If $a = 0$, $x = 0$ works.
        So assume $a > 0$.

        Look at $A = \{x \in \R\,|\, x ^ p < a\}$
        $0 \in A$, so $A$ is non-empty.

        Bounded above: If $a \geq 1$ then $x ^ p < a \leq a ^ p$,
        so $x ^ p < a ^ p$ and hence $x < a$ (If $x \geq 0$). We can use $a$ as the upper bound.
        If $a \in (0, 1)$, then $x ^ p < 1 \implies x < 1$, here we can use $1$ as an upper bound.
        
        By Completeness axiom, the $\sup A$ exists. 
        
        Call $\xi = \sup A$.
        We want $\xi ^ p = a$.
        To show: $\xi ^ p$ is not less than $a$ and not bigger than $a$.
        So assume $\xi ^ p < a$, note $\xi \in A$.
        Look at $\xi + \frac{1}{n}$ with $n \in \N$.
        \begin{align*}
            \left(\xi + \frac{1}{n}\right) ^ p &= \xi ^ p + \frac{1}{n} \cdot \binom{p}{1} \xi ^ {p - 1} + \frac{1}{n ^ 2} \cdot \binom{p}{2} \xi ^ {p - 2} + \dotsi + \frac{1}{n ^ p}\footnotemark \\
            &\leq \xi ^ p + \frac{1}{n}\left(\binom{p}{2}\xi ^ {p - 1} + \binom{p}{2}\xi ^ {p - 2} + \dotsi + 1\right) \\
            &= \xi ^ p + \frac{\alpha}{n}\quad\text{ with } \alpha > 0
        \end{align*}
        \footnotetext{By \autoref{analy:thm:binom}.}
        We want $\xi ^ p + \frac{\alpha}{n} < a \iff \frac{\alpha}{n} < a - \xi ^ p \iff \alpha < \left(a - \xi ^ p\right) \cdot n$ By \autoref{analy:thm:archim} there exists such an $n \in \N$
        so far such $n$
        \[
        \left(\xi + \frac{1}{n}\right) ^ p < a
        \]
        and therefore $\xi + \frac{1}{n} \in A$ contradicting $\xi$ is an upper bound for $A$.

        So assume $\xi ^ o > a$.
        Look at $\xi - \frac{1}{n}$ with $n \in \N$.
        \begin{align*}
            \left(\xi - \frac{1}{n}\right) ^ p &= \left(\xi \cdot\left(1 - \frac{1}{\xi n}\right)\right) ^ p \\
            &= \xi ^ p \left(1 - \frac{1}{\xi n}\right) ^ p \\
            &\geq \xi ^ p \left(1 - \frac{p}{\xi n}\right)\footnotemark \\
            &= \xi ^ p - \frac{\xi ^ {p - 1}p}{n}
        \end{align*}
        \footnotetext{By \autoref{analy:thm:bernoineq}}
        Want $\xi ^ p - \frac{\xi ^ {p - 1}p}{n} > a \iff (\xi ^ p - a) > \frac{\xi ^ {p - 1}p}{n} \iff n \cdot (\xi ^ p - a) > \xi ^ {p - 1} \cdot p \in \R$.
        By \autoref{analy:thm:archim} there exists such $n \in \N$.
        Contradicting $\xi$ being the supremum (We showed that $\xi - \frac{1}{n}$ is also an upper bound of $A$).
        By \autoref{analy:axiom:trichotomy} axiom $\xi ^ p = a$.
        \end{proof}
\end{theorem}

\begin{definition}
    Let $a > 0$ and $r = \frac{p}{q}$ with $p, q \in \N$. Then define
    \[
    a ^ r = \sqrt[q]{a ^ p}\qquad a ^ {-r} = \frac{1}{a ^ r}
    \]
    we also set $a ^ 0 = 1$.
\end{definition}

\begin{definition}
    Let $x \subset \R$ and $f : X \to \R$ a function.
    If $f(x)$ is bounded above,
    denote by $\sup(f)$ the supremum of this set $f(x)$.

    If $f(x)$ is bounded below,
    denote by $\inf(f)$ the infimum of $f(x)$.
\end{definition}

\newpage

\section{Sequences}

\subsection{Basics about sequences and limits}
\begin{definition}
    A sequence of real numbers is a function from $\N \to \R$.
\end{definition}

\begin{definition}
    A real sequence $\seq$ is said to be convergent to the limit $x \in \R$, if for every $\varepsilon > 0$, there exists an $n_0 \in \N$ with
    \[
    |x_n - x| < \varepsilon\text{ for all } n \geq n_0.
    \]
\end{definition}

\begin{theorem}[Uniqueness of the limit]
    A convergent sequence $\seq$ has precisely one limit.
    \begin{proof}
        Let $x, x'$ be limits of this sequence,
        with $x \neq x'$

        Let $\varepsilon = \frac{|x - x'|}{2} > 0$.

        Since $\seq$ converges to $x$,
        there is $n_0 \in \N$ with
        \[
        |x_n - x| < \varepsilon\text{ for all } n \geq n_0.
        \]
        We also have convergence to $x'$,
        there is $n_1 \in \N$, with
        \[
        |x_n - x'| < \varepsilon\text{ for all } n \geq n_1.
        \]
        For $n \geq \max\{n_0, n_1\}$ we now get
        \begin{align*}
            |x - x'| &= |x - x_n + x_n - x'| \\
            &\leq |x - x_n| + |x_n - x'| \\
            &< 2\varepsilon \\
            &= \frac{|x - x'|}{2} + \frac{|x - x'|}{2} \\
            &= |x - x'|
        \end{align*}
        Contradiction.
    \end{proof}
\end{theorem}

\begin{definition}
    Let $\seq$ be a real sequence,
    and denote $X$ the set $\{x_n \in \R \,|\, n \in \N\}$.
    The sequence $\seq$ is called bounded above,
    if $X \subset \R$ is bounded above.

    Bounded below, if $X \subset \R$ is bounded below.

    Bounded, if $X \subset \R$ is bounded above and below\footnote{$b_n = (-1) ^ n$ is bounded $X = \{-1, 1\}$.}.
\end{definition}

\begin{theorem}\label{analy:thm:convseqisbound}
    Every convergent sequence is bounded.
\end{theorem}

\begin{remark}
    A bounded sequence need not be convergent.
    e.g. $(-1) ^ n$
\end{remark}

\subsection{Convergence criteria}
\begin{theorem}[Squeezing Theorem]\label{analy:thm:squeezethm}
    Let $\seq$ and $\seq[y]$ be real sequences.
    If $|x_n| \leq y_n$ for all $n \in \N$ and $y_n \to 0$ as $n \to \infty$, then
    \[
    \lim_{n \to \infty}x_n = 0.
    \]
    \begin{proof}
        Let $\varepsilon > 0$ be given.
        Then there is $n_0 \in \N$ with
        \[
        |y_n - 0| = |y_n| < \varepsilon\text{ for all } n \geq n_0.
        \]
        Then
        \[
        |x_n - 0| = |x_n| \leq y_n = |y_n| < \varepsilon\text{ for all } n \geq n_0
        \]
        This means $x_n \to 0$ as $n \to \infty$.
    \end{proof}
\end{theorem}

\begin{theorem}[Calculus of Limits Theorem (COLT)]\label{analy:thm:COLT}
    Let $\seq$ and $\seq[y]$ be sequences which are convergent with $x = \lim_{n \to \infty}x_n$ and $y = \lim_{n \to \infty}$.
    Also, let $a, b \in \R$.
    Then
    \begin{enumerate}[label = (\alph*)]
        \item $ax_n + by_n \to ax + by$ as $n \to \infty$.
        \item $x_n \cdot y_n \to x \cdot y$ as $n \to \infty$.
        \item $\frac{x_n}{y_n} \to \frac{x}{y}$ as $n \to \infty$ provided that $y \neq 0$ and $y_n \neq 0\,\forall n \in \N$.
    \end{enumerate}
    \begin{enumerate}[label = (\alph*)]
        \item
        \begin{proof}
        \end{proof}
        \item 
        \begin{proof}
        We need to know that
        \begin{align*}
        |x_n \cdot y_n - x \cdot y| &= |x_n \cdot y_n - x_n \cdot y + x_n \cdot y - x \cdot y| \\
        &\leq |x_n (y_n - y)| + |(x_n - x)\cdot y| \\
        &= |x_n| \cdot |y_n - y| + |x_n - x| \cdot |y|
        \end{align*}
        Note,
        there is a $C > 0$ with $|x_n| \leq C$ by \autoref{analy:thm:convseqisbound}.
        Can also assume that $C \geq |y|$.
        Then we have
        \[
        |x_n \cdot y_n - x \cdot y| \leq C \cdot (|y_n - y| + |x_n - x|)
        \]
        Let $\varepsilon > 0$,
        there exists an $n_0$ with
        \[
        |y_n - y| < \frac{\varepsilon}{2C}\text{ for all } n \geq n_0
        \]
        and there exists an $n_1$ with
        \[
        |x_n - x| < \frac{\varepsilon}{2C}\text{ for all } n \geq n_1.
        \]
        Then for all $n \geq \max\{n_0, n_1\}$ we have
        \[
        |x_n \cdot y_n - xy| \leq C \cdot (|y_n - y| + |x_n - x|) < C \cdot (\frac{\varepsilon}{2C} + \frac{\varepsilon}{2C}) = 2C \cdot \frac{\varepsilon}{2C} = \varepsilon.
        \]
        \end{proof}
        \item
        \begin{proof}
        \end{proof}
    \end{enumerate}
\end{theorem}

\begin{theorem}\label{analy:thm:contofsqrt}
    Let $\seq$ be a sequence such that $x_n \in [a, b]$ for all $n$.
    If the sequence converges to $x \in \R$.
    Then $x \in [a, b]$.
    \begin{proof}
        Assume $x < a$ pick $\varepsilon - \frac{a - x}{2} > 0$.
        By convergence there is an $n_0 \in \N$ with
        \[
        |x_n - x| < \varepsilon \text{ for all } n \geq n_0.
        \]
        In particular
        $x - x_n > - \varepsilon$.
        Now $a - x_n = a - x + x - x_n = 2\varepsilon + x - x_n > \varepsilon > 0 \iff a > x_n$.
        Contradiction.
    \end{proof}
\end{theorem}

\begin{corollary}\label{analy:corol:seq:convgeq}
    Let $\seq$ and $\seq[y]$ be convergent sequences with $x_n \leq y_n$ for all $n \in \N$.
    Then
    \[
    \lim_{n \to \infty}x_n \leq \lim_{n \to \infty}y_n
    \]
    \begin{proof}
        Consider $a_n = y_n - x_n$.
        Then $a_n \geq 0$
        By the previous $\lim_{n \to \infty}a_n \geq 0$. By \autoref{analy:thm:COLT}
        \[
        \lim_{n \to \infty}y_n - \lim_{n \to \infty}x_n \geq 0.
        \]
    \end{proof}
\end{corollary}
\begin{theorem}
    Let $\seq$ be a convergent sequence with each $x_n \geq 0$.
    Then
    $(\sqrt{x_n})_{n \in \N}$ is a convergent sequence with
    \[
    \lim_{n \to \infty}\sqrt{x_n} = \sqrt{\lim_{n \to \infty}x_n}.
    \]
    \begin{proof}
        Denote $x = \lim_{n \to \infty}x_n \geq 0$ by \autoref{analy:corol:seq:convgeq}.
        Distinguish the cases $x = 0$ and $x > 0$.
        Assume $x > 0$.
        \begin{align*}
            |\sqrt{x_n} - \sqrt{x}| &= \left|\frac{(\sqrt{x_n} - \sqrt{x})(\sqrt{x_n} + \sqrt{x})}{\sqrt{x_n} + \sqrt{x}}\right| \\
            &= \frac{|x_n - x|}{\sqrt{x_n} + \sqrt{x}} \\
            &\leq \frac{|x_n - x|}{\sqrt{x}}
        \end{align*}
        by \autoref{analy:thm:squeezethm} $\sqrt{x_n} - \sqrt{x} \to 0$ and by \autoref{analy:thm:COLT} $\sqrt{x_n} \to \sqrt{x}$.
    \end{proof}
\end{theorem}

\begin{theorem}\label{analy:thm:boundmonoincseqconv}
    Let $\seq$ be a sequence which is monotonically increasing,
    that is,
    $x_m \leq x_n$ for $m \leq n$.
    If $\seq$ is bounded,
    then $\seq$ is convergent.
    \begin{proof}
        Look at $X = \{x_n \in \R | n \in \N\}$,
        this is a bounded set,
        and it is non-empty.
        By the completeness axiom $x = \sup X$ exists.
        To show $x = \lim_{n \to \infty}x_n$
        let $\varepsilon > 0$.
        Then $x - \varepsilon < x$,
        so $x - \varepsilon$ is not an upper bound for $X$.
        There is $x_{n_0} \in X$ with
        $x - \varepsilon < x_{n_0} \leq x_n \leq x < x + \varepsilon$ for all $n \geq n_0$
        this implies that
        $|x_n - x| < \varepsilon$ for all $n \geq n_0$.
    \end{proof}
\end{theorem}


\subsection{The exponential function and the logarithm}

\begin{theorem}
    For every $x \in \R$ the sequence $\left(1 + \frac{x}{n}\right) ^ n$ is convergent.
    The limit is denoted by $e ^ x$,
    it satisfies $e ^ x  > 0$ and
    \[
    e ^ {-x} = \frac{1}{e ^ x}.
    \]
    \begin{proof}[Proof sketch]\renewcommand{\qedsymbol}{$\triangle$}
        Proof for $e ^ {-x} = \frac{1}{e ^ x}$.

        \begin{align*}
            \left(1 + \frac{x}{n}\right) ^ n \cdot \frac{\left(1 - \frac{x}{n}\right) ^ n}{\left(1 - \frac{x}{n}\right) ^ n} &= \frac{\left(\left(1 + \frac{x}{n}\right)\left(1 - \frac{x}{n}\right)\right) ^ n}{\left(1 - \frac{x}{n}\right) ^ n} \\
            &= \frac{\left(1 - \frac{x ^ 2}{n ^ 2}\right) ^ n}{\left(1 + \frac{-x}{n}\right) ^ n}
        \end{align*}
        \[
        1 \geq \left(1 - \frac{x ^ 2}{n ^ 2}\right) ^ n \geq 1 - \frac{nx ^ 2}{n ^ 2} = 1 - \frac{x ^ 2}{n}
        \]
        By \autoref{analy:thm:bernoineq} for $n ^ 2 > x ^ 2$
        and by \autoref{analy:thm:squeezethm}
        \[
        \left(1 - \frac{x ^ 2}{n ^ 2}\right) ^ n \to 1
        \]
        then by \autoref{analy:thm:COLT} this implies
        \[
        e ^ x = \frac{1}{e ^ {-x}}.
        \]
    \end{proof}
\end{theorem}

\begin{lemma}\label{analy:lem:expboundlemma}
    Let $x \in (-\infty, 1)$.
    Then
    \[
    1 + x \leq e ^ x \leq \frac{1}{1 - x}.
    \]
    \begin{proof}
        Pick $n \in \N$ with $\frac{x}{n} > -1$.
        Then
        \[
        \left(1 + \frac{x}{n}\right) ^ n \geq 1 + \frac{nx}{n} = 1 + x
        \]
        By Thm 2.12.,
        $e ^ x \geq 1 + x$
        (works for all $x \in \R$).
        For the other inequality look at
        \[
        \frac{1}{\left(1 - \frac{x}{n}\right) ^ n} \leq \frac{1}{1 - x}
        \]
        By Thm 2.12.,
        $e ^ x \leq \frac{1}{1 - x}$.
    \end{proof}
\end{lemma}

\begin{theorem}
    Let $x, y \in \R$.
    Then
    \[
    e ^ {x + y} = e ^ x \cdot e ^ y.
    \]
\end{theorem}

\begin{theorem}
    Let $a > 0$.
    Then there exists a unique $x \in \R$ with $\exp(x) = a$.
    \begin{proof}
        Consider $X = \{y \in \R\,|\,\exp(y) < a\}$.
        For $n \in \N\ \exp(n) = e ^ n$,
        which gives an unbounded sequence.
        So there is $n \in \N$ with $e ^ n > a\ n \notin X$.
        
        Also,
        if $x < n$,
        $e ^ n < e ^ x\ x \notin X$.
        $n$ is an upper bound for $X$.
        $X$ is not empty.
        Look at $e ^ {-n} = \left(\frac{1}{e}\right) ^ n \to 0$.
        There is $n \in \N$ with $e ^ {-n} < a$,
        so $-n \in X$.
        Use $x = \sup X$ from the Completeness axiom.
        
        Show that $\exp(x) < a$ or $\exp(x) > a$ is not true.
        Assume $\exp(x) > a$.
        Look at $\exp\left(x - \frac{1}{n}\right)$ with $n \in \N$.
        \[
        \exp\left(x - \frac{1}{n}\right) = \frac{\exp(x)}{\exp\left(\frac{1}{n}\right)} \geq \frac{\exp(x)}{\frac{1}{1 - \frac{1}{n}}} = \exp(x) \cdot \left(1 - \frac{1}{n}\right)
        \]
        \[
        1 + x \leq e ^ x \leq \frac{1}{1 - x}\text{ on } \frac{1}{n}.
        \]
        So $\exp\left(x - \frac{1}{n}\right) \geq \exp x - \frac{\exp x}{n}$\quad(want $> a$).
        Choose $n$ so large that
        \[
        \frac{\exp x}{n} < \underbrace{\exp x}_{> 0} - a
        \]
        with such $n$ we have $\exp\left(x - \frac{1}{n}\right) > \exp x - (\exp x - a) = a \implies x - \frac{1}{n}$ is an upper bound for $X$. Contradicting $x$ was the supremum.
        So $\exp(x) > a$ is not true ($\exp(x) \leq a$).
    \end{proof}
\end{theorem}

\begin{definition}
    The logarithm function
    \[
    \log : (0, \infty) \to \R
    \]
    defined by
    $\log x = a$,
    where $a \in \R$ is such that $\exp(a) = x$.
\end{definition}

\begin{proposition}
    Let $a, b > 0$ with $b \neq 1$ and $x, y \in \R$.
    Then
    \begin{enumerate}[label = (\alph*)]
        \item $a ^ {x + y} = a ^ x \cdot a ^ y$
        \item $(a ^ x) ^ y = a ^ {x \cdot y}$
        \item $\log_b(xy) = \log_b x + \log_b y$ for $x, y > 0$
        \item $\log_b(x ^ y) = y \cdot \log_b(x)$ for $x > 0$
    \end{enumerate}
\end{proposition}

\begin{lemma}
    Let $x \in (0, \infty)$.
    Then 
    \[
    \frac{x - 1}{x} \leq \log(x) \leq x - 1.
    \]
    \begin{proof}
        
    \end{proof}
\end{lemma}

\subsection{The Bolzano-Weierstrass Theorem}
\begin{definition}
    Let $\seq$ be a sequence.
    A subsequence of $\seq$ is a sequence $(x_{n_j})_{j \in \N}$ with
    $n_1 < n_2 < n_3 < \dotsi$,
    $n_j$ is strictly monotonically increasing.
\end{definition}

\begin{proposition}
    Let $\seq$ be a convergent sequence with $\displaystyle\lim_{n \to \infty}x_n = x$.
    If $(x_{n_j})_{j \in \N}$ is a subsequence,
    then this is also convergent with $\displaystyle\lim_{j \to \infty}x_{n_j} = x$.
    \begin{proof}
        
    \end{proof}
\end{proposition}

\begin{lemma}
    Every real sequence $\seq$ contains a subsequence which is either increasing or decreasing.
    \begin{proof}
        Given a sequence $\seq$,
        call $n_0 \in \N$ a peak index,
        if $x_{n_0} \geq x_n$ for all $n > n_0$.
        Assume,
        our sequence has infinitely many peak indices
        $n_1 < n_2 < n_3 < \dotsi$,
        $x_{n_1} \geq x_{n_2} \geq x_{n_3} \geq x_{n_4} \geq \dotsi$ which is the required subsequence.
        Otherwise only finitely many peak indices.
        $n_1 < n_2 < \dotsi < n_k$ look at $n_{k + 1}$ it is not a peak index.
        So there is $n_{k + 2}$ with $x_{n_k + 1} < x_{n_k + 2}$
        $n_{k + 2}$ is also not a peak index,
        so there is $n_{k + 3} > n_{k + 2}$,
        $x_{n_k + 2} < x_{n_k + 3}$.
        Required subsequence $(x_{n_k + j})_{j \in N}$
    \end{proof}
\end{lemma}

\begin{theorem}[Bolzano-Weierstrass]\label{analy:thm:bolzanoweierstrass}
    Let $\seq$ be a bounded sequence.
    Then $\seq$ has a convergent subsequence.
    \begin{proof}
        Let $(x_{n_j})_{j \in \N}$ be a subsequence which is monotonically increasing or decreasing.
        This is also bounded.
        By \autoref{analy:thm:boundmonoincseqconv} $(x_{n_j})_{j \in \N}$ is convergent.
    \end{proof}
\end{theorem}

\subsection{Lim sup and Lim inf}

\begin{lemma}
    Let $\seq$ be a bounded sequence.
    Then $\seq[\overline{x}]$ is monotonically decreasing and bounded,
    and $\seq[\underline{x}]$ is monotonically increasing and bounded.
    Furthermore
    \[
    \lim_{n \to \infty}\underline{x_n} \leq \lim_{n \to \infty}\overline{x_n}.
    \]
    \begin{proof}
        $\seq$ is bounded,
        so there is $C \in \R$ with $-C \leq x_n \leq C$ for all $n \in \N$.
        Define $X_n = \{x_m \in \R\,|\, m \geq n\} \subseteq [-C, C]$ bounded,
        non-empty set.
        Infimum and supremum exist,
        and $-C \leq \underline{x_n} \leq \overline{x_n} \leq C$.
        both sequences are bounded.
        $X_{n + 1} \subset X_n \implies \overline{x_n}$ is an upper bound for $X_{n + 1} \implies \overline{x_{n + 1}} \leq \overline{x_n}$ for all $n \in \N$.
        Similarly,
        $\underline{x_n}$ is a lower bound for $X_{n + 1} \implies \underline{x_n} \leq \underline{x_{n + 1}}$ for all $n \in \N$ which implies $\overline{x_n} - \underline{x_n} \geq 0$.
        Taking limits gives
        \[
        \lim\overline{x_n} - \lim\underline{x_n} \geq 0.
        \]
    \end{proof}
\end{lemma}

\begin{definition}
    Let $\seq$ be a bounded sequence.
    The limes superior of $\seq$ is defined as
    \[
    \limsup_{n \to \infty}{x_n} = \lim_{n \to \infty}\overline{x}_n = \inf_{n \geq 1}\{\sup{\{x_m\,|\, m \geq n\}}\}.
    \]
    The lines inferior of $\seq$ is defined as
    \[
    \liminf_{n \to \infty}{x_n} = \lim_{n \to \infty}{\underline{x}_n} = \sup_{n \geq 1}{\{\inf{\{x_m\,|\, m \geq n\}}\}}.
    \]
\end{definition}

\begin{theorem}
    Let $\seq$ be a bounded sequence.
    \begin{enumerate}[label = \alph*)]
        \item There is a subsequence $(x_{n_j})_{j \in \N}$ with
        \[
        \lim_{j \to \infty}x_{n_j} = \limsup_{n \to \infty}{x_n}.
        \]
        \item There is a subsequence $(x_{n_k})_{k \in \N}$ with
        \[
        \lim_{k \to \infty}x_{n_k} = \liminf_{n \to \infty}{x_n}.
        \]
        \item If $(x_{n_j})_{j \in \N}$ is a convergent subsequence,
        then
        \[
        \liminf_{n \to \infty}{x_n} \leq \lim_{j \to \infty}{x_{n_j}} \leq \limsup_{n \to \infty}{x_n}
        \]
    \end{enumerate}
\end{theorem}

\subsection{Cauchy Sequence}
\begin{definition}
    A sequence $\seq$ is called a Cauchy sequence,
    if for every $\varepsilon > 0$ there exists $n_0 \in \N$ such that
    \[
    |x_m - x_n| < \varepsilon\qquad\forall n, m \geq n_0.
    \]
\end{definition}

\begin{theorem}\label{analy:thm:cauchyisbounded}
    Let $\seq$ be a Cauchy sequence.
    Then $\seq$ is bounded.
    \begin{proof}
        
    \end{proof}
\end{theorem}

\begin{theorem}
    Let $\seq$ be a convergent sequence.
    Then $\seq$ is a Cauchy sequence.
    \begin{proof}
        
    \end{proof}
\end{theorem}

\begin{theorem}
    Let $\seq$ be a Cauchy sequence.
    Then $\seq$ is a convergent sequence.
    \begin{proof}
        By \autoref{analy:thm:cauchyisbounded} $\seq$ is a bounded sequence.
        By \autoref{analy:thm:bolzanoweierstrass},
        there is a convergent subsequence $(x_{n_j})_{j \in \N}$.
        Let $x = \lim_{j \to \infty}x_{n_j}$.
        So given $\varepsilon > 0$,
        there is a $j_0 \in \N$ with
        \[
        |x_{n_j} - x| < \frac{\varepsilon}{2}\qquad\forall j \geq j_0.
        \]
        Since $\seq$ is a Cauchy sequence,
        there is an $n_0 \in \N$ with
        \[
        |x_n - x_m| < \frac{\varepsilon}{2}\qquad\forall n, m \geq n_0.
        \]
        Now
        \begin{align*}
            |x_n - x| &= |x_n - x_{n_j} + x_{n_j} - x| \\
            &\leq |x_n - x_{n_j}| + |x_{n_j} - x|.
        \end{align*}
        Need $j \geq j_0$ and $n_j \geq n_0$
        (just choose $j$ large enough).
        Then $|x_n - x| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon$ for all $n \geq n_0$.
    \end{proof}
\end{theorem}

\newpage

\section{Series}

\subsection{Fundamental notions and properties of series}

\begin{definition}
    Let $\dseq[k]{a}$ be a sequence of real numbers.
    Then the sequence of partial sums $\dseq{S}$ is defined as
    \[
    S_n = \sum_{k = 1}^{n}a_k.
    \]
    If the sequence of partial sums converges,
    we say the series $\sum_{k = 1}^{\infty}a_k$ is convergent,
    and then $\sum_{k = 1}^{\infty}a_k = \lim_{n \to \infty}S_n$.
    Otherwise,
    we say $\sum_{k = 1}^{\infty}a_k$ is divergent.
\end{definition}

\begin{remark}
    Series can start with a summand different from $a_1$,
    e.g.
    $\infsum[k = 0]a_k$ or $\infsum[k = 3]a_k$.
    Make sure all the terms are defined.
\end{remark}

\begin{lemma}
    \[
    \infsum a_k\text{ converges } \iff \infsum[k = N]a_k\text{ converges } (N \in \N).
    \]
\end{lemma}

\begin{lemma}
    If $\infsum[k = 0]a_k$ is convergent,
    then
    \[
    \lim_{k \to \infty}a_k = 0.
    \]
    \begin{proof}
        Let $S_n = \sum_{k = 0}^{n}a_k$,
        then $\lim_{n \to \infty}S_n = s$ for some $s \in \R$.
        Write $a_n = S_n - S_{n - 1}$ for $n \geq 1$.
        By COLT $\lim_{n \to \infty}a_n = s - s = 0$.
    \end{proof}
\end{lemma}

\begin{theorem}[COLT for series]
    Assume the series $\infsum[k = 0]a_k$ and $\infsum[k = 0]b_k$ are convergent with limit $a$ and $b$
    (respectively).
    Then
    \begin{enumerate}[label = (\alph*)]
        \item
        \[
        \infsum[k = 0](a_k + b_k) \text{ converges to } a + b
        \]
        \item
        \[
        \infsum[k = 0]c\cdot a_k\text{ converges to } c \cdot a, \text{ where } c \in \R.
        \]
    \end{enumerate}
    \begin{proof}
        Use COLT for sequences on partial sums.
    \end{proof}
\end{theorem}

\subsection{Convergence Criteria}
\begin{theorem}[Comparison Test]
    Let $N \in \N$ let $(a_k)_{k \in \N_0}$, $(b_k)_{k \in \N_0}$ sequences with
    \[
    0 \leq a_k \leq b_k \text{ for } k \geq N.
    \]
    \begin{enumerate}[label = (\alph*)]
        \item If $\infsum[k = 0]b_k$ is convergent,
        then $\infsum[k = 0]a_k$ is convergent.
        \item If $\infsum[k = 0]a_k$ is divergent,
        then $\infsum[k = 0]b_k$ is divergent.
    \end{enumerate}
    \begin{proof}
        Use $\infsum[k = 0]a_k$ is convergent if and only if $\infsum[k = N]a_k$ is convergent.

        For $n \geq N$ let $s_n = \sum_{k = N}^{n}a_k$ and $t_n = \sum_{k = N}^{n}b_k$.
        Theses are monotonically increasing.

        If $\infsum[k = 0]b_k$ is convergent so is $(t_n)_{n \geq N}$ to $t$.
        \[
        s_n \leq t_n \leq t.
        \]
        The sequence $s_n$ is monotonically increasing by $t$,
        so is convergent by Thm 2.16.

        (b)
        Since $s_n$ is monotonically increasing,
        divergence means $(s_n)$ is unbounded,
        but then $(t_n)$ is also unbounded.
    \end{proof}
\end{theorem}

\begin{theorem}
    Let $\alpha \in \R$.
    Then
    \[
    \infsum\frac{1}{k ^ \alpha}\text{ is convergent, if and only if $\alpha > 1$}.
    \]
    \begin{proof}
        For $\alpha = 1$ we know it is divergent,
        by comparison test we also get divergence for $\alpha \leq 1$.

        For $\alpha > 1$ we need convergence.
        Look at sequence of partial sums $\displaystyle S_n = \sum_{k = 1}^{n}\frac{1}{k ^ \alpha}$.
        This sequence is monotonically increasing.
        \begin{align*}
            S_{2n + 1} &= \sum_{k = 1}^{2n + 1}\frac{1}{k ^ \alpha} \\
            &= 1 + \sum_{k = 1}^{n}\left(\frac{1}{(2k) ^ \alpha} + \frac{1}{(2k + 1) ^ \alpha}\right) \\
            &\leq 1 + \sum_{k + 1}^{n}\frac{2}{(2k) ^ \alpha} \\
            &= 1 + \sum_{k = 1}^{n}\frac{2 ^ {1 - \alpha}}{k ^ \alpha} \\
            &= 1 + 2 ^ {1 - \alpha}\sum_{k = 1}^{n}\frac{1}{k ^ \alpha} \\
            &\leq 1 + 2 ^ {1 - \alpha}S_n \\
            &\leq 1 + 2 ^ {1 - \alpha}S_{2n + 1}
        \end{align*}
        So
        \begin{align*}
            S_{2n + 1} \leq 1 + \underbrace{2 ^ {1 - \alpha}}_{< 1}S_{2n + 1} &\iff (1 - 2 ^ {1 - \alpha})S_{2n + 1} \leq 1 \\
            &\iff S_{2n + 1} \leq \frac{1}{1 - 2 ^ {1 - \alpha}}.
        \end{align*}
        Since the sequence is monotonically increasing,
        it is now also bounded.
        By \autoref{analy:thm:boundmonoincseqconv} the sequence of partial sums is convergent.
        
        Hence $\displaystyle\infsum[n = 1]\frac{1}{n ^ \alpha}$ is convergent for $\alpha > 1$.
    \end{proof}
\end{theorem}

\begin{definition}
    A series $\infsum a_k$ is called alternating,
    if
    \[
    a_{2k} \geq 0\text{ and } a_{2k - 1} \leq 0\text{ for all $k \in \N$}
    \]
    or if 
    \[
    a_{2k} \leq 0\text{ and } a_{2k - 1} \geq 0\text{ for all $k \in \N$}.
    \]
\end{definition}

\begin{theorem}[Alternating Sign Test]
    Let $\dseq[k]{a}$ be a monotonically decreasing sequence of positive numbers with $\lim_{k \to \infty}a_k = 0$.
    Then the alternating series $\infsum(-1) ^ {k + 1}a_k$ is convergent.

    For the sequence of partial sums $\dseq{s}$ we have
    \[
    S_2 \leq S_4 \leq \dotsi \leq S_{2n} \leq \dotsi \leq \infsum(-1) ^ {k + 1}a_k \leq \dotsi \leq S_{2n - 1} \leq \dotsi \leq S_3 \leq S_1
    \]
    and
    \[
    \left|S_n - \infsum(-1) ^ {k + 1}a_k\right| \leq a_{n + 1}.
    \]
    \begin{proof}
        Since the $a_k$ are monotonically decreasing,
        we have $a_k - a_{k + 1} \geq 0$.
        Hence $S_{2n + 2} = S_{2n} + a_{2n + 1} - a_{2n + 2} \geq S_{2n}$.
        
        Similarly
        \[
        S_{2n + 1} = S_{2n - 1} - a_{2n} + a_{2n + 1} \leq S_{2n - 1}
        \]
        so $(S_{2n})_{n \in \N}$ monotonically increasing,
        and $(S_{2n - 1})_{n \in \N}$ monotonically decreasing.
        We get
        \[
        S_2 \leq S_{2n} \leq S_{2n - 1} \leq S_1
        \]
        sequences are bounded,
        hence convergent.
        Write $S ^ e = \lim_{n \to \infty}S_{2n}$
        $S ^ o = \lim_{n \to \infty}S_{2n - 1}$.
        Note $S_{2n} = S_{2n - 1} - a_{2n}$
        that is,
        $a_{2n} = S_{2n - 1} - S_{2n}$
        which converges to $0 = S ^ o - S ^ e$
        and by COLT
        $S ^ o = S ^ e$.

        Let $\varepsilon > 0$.
        We have $n ^ e \in \N$ with
        \begin{equation}\tag{*}
            |S_{2n} - s| < \varepsilon
        \end{equation}
        whenever $2n \geq n ^ e$ and there exist $n ^ o \in \N$ with
        \begin{equation}\tag{**}
            |S_{2n - 1} - s| < \varepsilon
        \end{equation}
        whenever $2n - 1 \geq n ^ o$.
        Take $n_0 = \max\{n ^ e, n ^ o\}$.
        For $n \geq n_0$ we get
        \[
        |S_n - s| < \varepsilon,
        \]
        since $n$ is either even or odd,
        but since $n \geq n ^ e$ and $n \geq n ^ o$ we can use either inequality (*) or (**).
    \end{proof}
\end{theorem}

\subsection{Absolute Convergence}
\begin{definition}
    Let $\displaystyle\infsum[k = 1]a_k$ be a series,
    we call it absolutely convergent,
    if $\displaystyle\infsum[k = 1]|a_k|$ is convergent.
\end{definition}

\begin{theorem}
    Let $\infsum[k = 1]a_k$ be absolutely convergent.
    Then the series is convergent.
    \begin{proof}
        We have $\infsum[k = 1]|a_k|$ is convergent.
        By COLT
        \[
        \infsum[k = 1]2|a_k|\text{ is convergent}.
        \]
        We have $-|a_k| \leq a_k \leq |a_k|$.
        So $0 \leq |a_k| + a_k \leq 2|a_k|$.
        By the comparison test,
        \[
        \infsum[k = 1](a_k + |a_k|)
        \]
        is convergent.
        By COLT
        \[
        \infsum[k = 1]a_k = \infsum[k = 1]((a_k + |a_k|) - |a_k|)
        \]
        is convergent.
        Additionally,
        \[
        \infsum[k = 1]a_k \leq \infsum[k = 1]|a_k|
        \]
        we have
        \[
        \sum_{k = 1}^{n}a_k \leq \sum_{k = 1}^{n}|a_k|.
        \]
    \end{proof}
\end{theorem}

\begin{theorem}[Ratio Test]
    Let $\dseq[k]{a}$ be a sequence with $a_k \neq 0$ for all $k \in \N$ except finitely many.
    \begin{enumerate}[label = (\alph*)]
        \item If $\displaystyle\lim_{k \to \infty}\left|\frac{a_{k + 1}}{a_k}\right| < 1$,
        then the series $\displaystyle\infsum[k = 1]$ is absolutely convergent.
        \item If $\displaystyle\lim_{k \to \infty}\left|\frac{a_{k + 1}}{a_k}\right| > 1$,
        then the series $\displaystyle\infsum[k = 1]a_k$ is divergent.
    \end{enumerate}
\end{theorem}

\begin{remark}
    The limit $\displaystyle\lim_{k \to \infty}\left|\frac{a_{k + 1}}{a_k}\right|$ need not exist.
    But we can still make a conclusion,
    \begin{enumerate}[label = (\alph*')]
        \item If $\displaystyle\limsup_{k \to \infty}\left|\frac{a_{k + 1}}{a_k}\right| < 1$,
        that is $\left|\frac{a_{k + 1}}{a_k}\right| \leq q < 1$ for all finitely many $k$,
        then the series is absolutely convergent.
        \item If $\left|\frac{a_{k + 1}}{a_k}\right| \geq 1$ for all but finitely many $k$,
        then $\displaystyle\infsum[k = 1]a_k$ diverges.
    \end{enumerate}
    \begin{proof}
        Assume (a').
        There exists $n_0$ with
        \begin{align*}
            &\left|\frac{a_{k + 1}}{a_k}\right| \leq q\text{ for all } k \geq n_0 \\
            &\implies |a_{k + 1}| \leq |a_k|\cdot q \\
            &\implies |a_{n_0 + j}| \leq |a_{n_0 + j - 1}| \cdot q \leq |a_{n_0 + j - 2}|\cdot q ^ 2 \leq \dotsc \leq |a_{n_0}|q ^ {j}.
        \end{align*}
        $0 \leq q < 1$,
        so
        \[
        \infsum[j = 0]|a_{n_0}| \cdot q ^ j = |a_{n_0}|\frac{1}{1 - q}
        \]
        \[
        \infsum[j = 0]|a_{n_0 + j}|
        \]
        is convergent by comparison test.
        \[
        \infsum[j = 0]|a_{n_0 + j}| = \infsum[k = n_0]|a_k|
        \]
        so this is absolutely convergent.
        Hence $\displaystyle\infsum[k = 1]|a_k|$ is absolutely convergent.
        
        \textit{Note:}
        If $\lim_{k \to \infty}\frac{|a_{k + 1}|}{|a_k|} = q > 1$
        look at $\varepsilon = q - 1 > 0$.
        
        Then we have $n_0 \in \N$ with
        \[
        \left|\left|\frac{a_{k + 1}}{a_k}\right| - q\right| < \varepsilon
        \]
        for all $k \geq n_0$
        so
        \[
        -\varepsilon < \left|\frac{a_{k + 1}}{a_k}\right| - q < \varepsilon = q - 1
        \]
        so
        \[
        \left|\frac{a_{k + 1}}{a_k}\right| > 1
        \]
        for all $k \geq n_0$.
        Proof of (b'):
        
        There is $n_0 \in \N$ with $\left|\frac{a_{k + 1}}{a_k}\right| \geq 1$ for all $k \geq n_0$
        which implies $|a_{k + 1} \geq |a_k| > 0$.
        In particular,
        $\dseq[k]{a}$ does not converge to $0$,
        the series is therefore divergent.
    \end{proof}
\end{remark}

\begin{theorem}[Root test]
    For a sequence $\dseq[k]{a}$ set
    \[
    a = \limsup_{k \to \infty}\sqrt[k]{|a_k|}
    \]
    \begin{enumerate}[label = (\alph*)]
        \item If $a < 1$,
        then $\displaystyle\infsum[k = 1]a_k$ is absolutely convergent.
        \item If $a > 1$,
        then $\displaystyle\infsum[k = 1]a_k$ is divergent.
    \end{enumerate}
    \begin{proof}
        Assume $a < 1$.
        Then for all but finitely many $k$ we have
        \[
        \sqrt[k]{|a_k|} \leq q < 1
        \]
        use $q = \frac{a + 1}{2}$.
        So for all $k \geq n_0$ we have $|a_k| \leq q ^ k$.
        By comparison test with the convergent geometric series
        \[
        \infsum[k = 1]q ^ k
        \]
        we get absolute convergence.

        Assume $a > 1$,
        then for all but finitely many $k$,
        $\sqrt[k]{|a_k|} \geq q > 1$.
        hence for all $k \geq n_1$ $|a_k| \geq q ^ k$.
        By the comparison test with the diverging geometric series
        \[
        \infsum[k = 1]q ^ k
        \]
        we get divergence.
    \end{proof}
\end{theorem}

\subsection{Rearrangements of Series}

\begin{definition}
    Let $\infsum a_k$ be a series.
    We say this series is conditionally convergent,
    if it is convergent,
    but not absolutely convergent.
\end{definition}

\begin{theorem}[Riemann Rearrangement Theorem]
    Let $\infsum a_k$ be a conditionally convergent series,
    and $L \in \R$.
    Then there exists a rearrangement
    \[
    \sigma : \N \to \N
    \]
    (which is injective and surjective)
    such that the rearranged sum
    \[
    \infsum a_{\sigma(k)}
    \]
    converges to $L$.
    Moreover,
    there is a rearrangement that is divergent.
    \begin{proof}
        Let
        \[
        a_k^{+} = \frac{|a_k| + a_k}{2} = \begin{cases}
            |a_k| & \text{if } a_k \geq 0 \\
            0 & \text{if } a_k < 0.
        \end{cases}
        \]
        Let
        \[
        a_k^{-} = \frac{|a_k| - a_k}{2} = \begin{cases}
            0 & \text{if } a_k \geq 0 \\
            |a_k| & \text{if } a_k < 0.
        \end{cases}
        \]
        Both $\infsum a_k^{+}$ and $\infsum a_k^{-}$ are divergent:
        \[
        a_k = a_k^{+} - a_k^{-}
        \]
        \[
        |a_k| = a_k^{+} + a_k^{-}.
        \]
        So if one of them were convergent,
        so would be the other
        (If $\infsum a_k^{+}$ is convergent,
        $a_k^{-} = a_k^{+} - a_k$ and $\infsum a_k^{-}$ is convergent by COLT).
        If both are convergent,
        series is absolutely convergent.

        Now let $b_k$ be the $k$th element of the sequence $\seq[a]$ which is $\geq 0$,
        and $c_k$ is the $k$th element which is $< 0$.
        Then $\infsum b_k$ and $\infsum c_k$ are also divergent.
        Note $\sum_{k = 1}^{n}c_k = -\sum_{k = 1}^{k_n}a_k^{-}$
        $k_n$ the $n$the element which gives $a_{k_n} < 0$.

        Given $L \in \R$,
        let $n_0$ be the first index with
        \[
        \sum_{k = 1}^{n_0}b_k > L
        \]
        (if $L < 0,\ n_0 = 0$ will work).

        Now let $n_1$ be the first index such that
        \[
        \sum_{k = 1}^{n_0}b_k + \sum_{k = 1}^{n_1}c_k < L.
        \]
        Now pick $n_2 > n_1$ such that
        \[
        \sum_{k = 1}^{n}b_k + \sum_{k = 1}^{n_1}c_k + \sum_{k = n_0 + 1}^{n_2}b_k > L.
        \]
        Continue.
        We have $\lim b_k = \lim c_k = 0$.
        and this allows us to get convergence to $L$ with the right arrangement.
    \end{proof}
\end{theorem}

\begin{theorem}
    Let $\infsum a_k$ be an absolutely convergent series and $\sigma : \N \to \N$ be a bijection.
    Then $\infsum a_{\sigma (k)}$ is also absolutely convergent,
    and we have
    \[
    \infsum a_k = \infsum a_{\sigma(k)}.
    \]
    \begin{proof}
        Look at $s_n = \sum_{k = 1}{n}|a_k|$ monotonically increasing sequence which converges to $a$,
        and so $s_n \leq a$.
        For $s'_n = \sum_{k = 1}^{n}|a_{\sigma(k)}| \leq s_m$ where $m = \max\{\sigma(1), \dotsc, \sigma(n)\}$ which implies $s'_n \leq a$ for all $n$,
        so $\seq[s']$ is a convergent series shows absolute convergence of $\infsum a_{\sigma(k)}$ with limit $a' = \infsum|a_{\sigma(k)}|$.
        Note that $\infsum|a_k|$ is a rearrangement of $\infsum|a_{\sigma(k)}|$
        (using $\sigma ^ {-1}$) which implies $a \leq a' \leq a$ which implies $a = a'$.

        Look at $b_k = a_k + |a_k| \geq 0$.
        Then
        \[
        \infsum a_k + |a_k| = \infsum a_{\sigma(k)} + |a_{\sigma(k)}|.
        \]
        By COLT
        \[
        \infsum a_k + \infsum|a_k| = \infsum a_{\sigma(k)} + \infsum|a_{\sigma(k)}|
        \]
        which implies $\infsum[k = 1]a_k = \infsum a_{\sigma(k)}$.
    \end{proof}
\end{theorem}

\begin{theorem}[Cauchy Product Theorem]
    Let $\infsum[k = 0]a_k$ and $\infsum[k = 0]b_k$ be two absolutely convergent series with limits $a, b \in \R$,
    respectively.
    For $n \geq 0$,
    let
    \[
    c_n = \sum_{k = 0}^{n}a_kb_{n - k}.
    \]
    Then the series $\infsum[k = 0]c_k$ is called the Cauchy Product of $\sum a_k$ and $\sum b_k$.
    The series $\sum c_k$ is also absolutely convergent and we have
    \[
    \infsum[k = 0]c_k = a \cdot b.
    \]
\end{theorem}

\newpage

\section{Functions, Limits, and Continuity}

\subsection{Basics}

\begin{definition}[Open interval]
    \[
    x \in (a, b) \iff a < x < b
    \]
    \textit{$a = -\infty, b = \infty$ is fine}.
\end{definition}

\begin{definition}[Open set]
    Given $X \subseteq \R$. For all $c \in X$ there exists an open interval in $X$ containing $c$.
    
    That is,
    there exists $\delta > 0$,
    such that
    \[
    (c - \delta, c + \delta) \subseteq X.
    \]
\end{definition}

\begin{definition}[Interior point]
    An interior point $c \in X$ if there exists $(c - \delta, c + \delta) \subset X$.
\end{definition}

\begin{definition}[Closed interval]
    \[
    x \in [a, b] \iff a \leq x \leq b
    \]
    ($a, b \in \R$).
\end{definition}

\begin{lemma}\label{analy:lem:limitliesinab}
    $(x_n) \in [a, b]$ converging with $\liminfty x_n = L$.
    Then $L \in [a, b]$.
    \begin{proof}
        Assume $L \notin [a, b]$;
        take $\varepsilon = \min\{|L - b|, |L - a|\}$.
        Say $L > b$.
        For all $x_n$ we have
        \begin{align*}
            |x_n - L| &= |x_n - b + b - L| \\
            &= (b - x_n) + (L - b) \\
            &> \varepsilon.
        \end{align*}
        Contradiction to $\liminfty x_n = L$.
    \end{proof}
\end{lemma}

\begin{remark}
    The previous is false for open intervals.
    For example,
    \[
    x_n = \frac{1}{n} > 0\qquad(a, b) = (0, 2).
    \]
    But $\liminfty x_n = 0 \notin (0, 2)$.
\end{remark}

\begin{theorem}[Bolzano-Weierstrass]
    $(x_n) \in [a, b]$,
    with $a, b \in \R$,
    has a converging subsequence converging in $[a, b]$.
    \begin{proof}
        $(x_n)$ are bounded which by \autoref{analy:thm:bolzanoweierstrass} has a convergent subsequence and by \autoref{analy:lem:limitliesinab}.
    \end{proof}
\end{theorem}

\begin{definition}[Compact interval]
    Call $[a, b]$
    ($a, b \in \R$)
    a compact interval if the interval is bounded and closed.
\end{definition}

\subsection{Limits of functions}

\begin{definition}
    Let $f : (a, b) \to \R$ be a function.
    Let $c \in (a, b)$ and $f$ is possibly not defined at $c$.
    We say
    \[
    \lim_{x \to c}f(x) = L
    \]
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - L| < \varepsilon
    \]
    for all $x \neq c$ with
    \[
    |x - c| < \delta.
    \]
    We also write $f(x) \to L$ as $x \to c$.
\end{definition}

\begin{remark}
    \begin{enumerate}[label = (\roman*)]
        \item $\delta$ is not unique,
        any smaller positive number will work.

        \item The property does not depend on the interval $(a, b)$,
        $\lim_{x \to c}f(x)$ is a local property,
        only depending on any small open interval around $c$,
        $(c - \delta, c + \delta)$.

        \item $\delta$ will depend on $c$.

        \item $L$ might or might not be equal to $f(c)$ if $f$ is defined at $c$.
    \end{enumerate}
\end{remark}

One sided limits

\begin{definition}[Limit from the right]
    \[
    \lim_{x \to c ^ {+}}f(x) \text{ same but all } x > c.
    \]
\end{definition}


\begin{definition}[Limit from the left]
    \[
    \lim_{x \to c ^ {-}}f(x) \text{ same but all } x < c.
    \]
\end{definition}

\begin{definition}[Infinite limit]
    $\liminfty[x] f(x) = L$:
    for all $\varepsilon > 0$ there exists a $k \in \R$ such that
    \[
    |f(x) - L| < \varepsilon\qquad\text{for all } x > k.
    \]
\end{definition}

\begin{proposition}
    \[
    \lim_{x \to c}f(x) = L
    \]
    \[
    \iff
    \]
    for all sequences $(x_n)$ with $\liminfty x_n = c$ have $\liminfty f(x_n) = L$.
    \begin{proof}
        "$\implies$".
        Assume $\lim_{x \to c}f(x) = L$.
        Take $(x_n) \in (a, b)$ $(x_n \neq c)$ with $\liminfty x_n = c$.
        Take $\varepsilon > 0$.
        Need an $N$ such that
        \[
        |f(x_n) - L| < \varepsilon
        \]
        for all $n \geq N$.
        We know there exists a $\delta > 0$ such that
        \begin{equation}\label{analy:eq:1}
            |f(x) - L| < \varepsilon
        \end{equation}
        for all $|x - c| < \delta$ $(x \neq c)$.
        Since $\liminfty x_n = c$ there exists an $N$ such that $|x_n - c| < \delta$ for all $n \geq N$.
        By \eqref{analy:eq:1} $|f(x_n) - L| < \varepsilon$ for all $n \geq N$.

        "$\impliedby$".
        By contrapositive.
        Assume $\lim_{x \to c}f(x) \neq L$
        (or does not exist).
        Need to find a sequence $x_n$ where $\liminfty x_n = c$ but $\liminfty f(x_n) \neq L$
        (or does not exist).
        Hence there exists a $\varepsilon > 0$ such that for all $\delta > 0$ such that there exists an $x$ with $|x - c| < \delta$ but $|f(x) - L| \geq \varepsilon$.
        Take the "bad" $\varepsilon > 0$.
        Take $\delta = 1 / n$,
        get an $x = x_n$ with $|x_n - c| < \delta = \frac{1}{n}$ but $|f(x_n) - L| \geq \varepsilon$.
        \[
        \liminfty x_n = c
        \]
        but
        \[
        \liminfty f(x_n) \neq L.
        \]
        This completes the proof by the contrapositive.
    \end{proof}
\end{proposition}

\begin{proposition}[COLT]
    We have $\lim_{x \to c}f(x) = L_1$,
    $\lim_{x \to c}g(x) = L_2$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item
        \[
        \lim_{x \to c}(af(x) + bg(x)) = aL_1 + bL_2.
        \]

        \item
        \[
        \lim_{x \to c}(f(x)g(x)) = L_1L_2.
        \]

        \item If $L_2 \neq 0$
        \[
        \lim_{x \to c}\left(\frac{f(x)}{g(x)}\right) = \frac{L_1}{L_2}
        \]
    \end{enumerate}

    \begin{proof}
        Using the previous proposition,
        and apply COLT for sequences.
        \begin{enumerate}[label = (\roman*)]
            \item \phantom{}
            
            \item Take $x_n \to c$
            \[
            \liminfty\left[f(x_n)g(x_n)\right]
            \]
            
            by COLT for sequences
            \[
            \liminfty\left[f(x_n)g(x_n)\right] = \lim_{x \to c}f(x_n)\cdot\liminfty g(x_n) = L_1L_2.
            \]
            
            \item \phantom{}
        \end{enumerate}
    \end{proof}
\end{proposition}

\begin{proposition}[Squeezing]
    Assume $f(x) \leq g(x) \leq h(x)$.
    For all $x$ in a neighbourhood\footnote{Close to $c$.} of $c$ with
    \[
    \lim_{x \to c}f(x) = \lim_{x \to c}h(x) = L.
    \]
    Then
    \[
    \lim_{x \to c}g(x) = L.
    \]
\end{proposition}

\subsection{Continuous functions}

\begin{definition}[Co]
    $f : X \to \R$,
    $X = (a, b)$
    $c \in (a, b)$.
    Call $f(x)$ continuous at $x = c$ if
    \[
    \lim_{x \to c}f(x) = f(c).
    \]
    For all $\varepsilon > 0$,
    there exists $\delta > 0$ such that
    \[
    |f(x) - f(c)| < \varepsilon
    \]
    for all $x$ with
    \[
    |x - c| < \delta.
    \]
\end{definition}

\begin{remark}\phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item Continuity is a local property,
        only depending on the behaviour of $f(x)$ in a potentially small neighbourhood of $c$.
        Can restrict $f(x)$ to such a neighbourhood if necessary.

        \item Can define left and right continuity as well as before
        \[
        \lim_{x \to c ^ {\pm}}f(x) = f(c).
        \]
        \textit{Notion:
        get continuity on $[a, b]$}.
    \end{enumerate}
\end{remark}

\begin{proposition}
    $f(x)$ is continuous at $x = c$ if and only if
    \[
    \liminfty f(x_n) = f\left(\liminfty x_n\right).
    \]
\end{proposition}

\begin{theorem}[Continuous COLT]
    $f, g$ are continuous at $x = c$.
    \begin{enumerate}[label = (\roman*)]
        \item
        \[
        af(x) + bg(x) \text{ is continuous at } c.
        \]

        \item
        \[
        f(x)g(x) \text{ is continuous at } c.
        \]

        \item If $g(c) \neq 0$
        \[
        \frac{f(x)}{g(x)} \text{ is continuous at } c.
        \]
    \end{enumerate}
\end{theorem}

\begin{proposition}
    Assume $f$ is continuous at $c \in X$ and $g$ is continuous at $f(c) \in Y$.
    Then $g \circ f(x)$ is continuous at $x = c$.
    \begin{proof}
        Use the sequence criterion:
        take $x_n \in X$ with $\liminfty x_n = c$.
        Need $\liminfty g \circ f(x_n) = g(f(c))$.
        
        Set $y_n = f(x_n)$ since $f$ is continuous at $c$ we have that $\liminfty f(x_n) = \liminfty y_n = f(c)$,
        this sequence,
        $f(x_n)$,
        is in $Y$.
        Since $g$ is continuous at $f(c)$ we have $\liminfty g(y_n) = \liminfty g(f(c)) = \liminfty g(f(x_n))$.
    \end{proof}
\end{proposition}

\subsection{Great Theorems}
\begin{theorem}[Intermediate Value Theorem]
    $f : [a, b] \to \R$ continuous.
    with $f(a) < f(b)$
    (say).
    Pick $d \in [f(a), f(b)]$;
    $f(a) \leq d \leq f(b)$.
    Then there exists a $c \in [a, b]$
    (not necessarily unique)
    such that $f(c) = d$.
    \begin{proof}
        Pick $d$,
        assume $d < f(b)$
        (otherwise can pick $c = b$).
        
        Define the set
        \[
        X \coloneqq \{x \in [a, b]; f(x) \leq d\}.
        \]
        $X \neq \emptyset$,
        since $a \in X$ and bounded as a subset of $[a, b]$.
        Hence has a supremum,
        $c$.
        (By term $1$)
        exists a sequence $x_n \in X$ such that $\liminfty x_n = c$.
        $x_n \in X \subseteq [a, b]$ hence $c = \liminfty x_n \in [a, b]$.
        By continuity $\liminfty f(x_n) = f\left(\liminfty x_n\right) = f(c)$.

        Claim:
        $f(c) = d$.

        Assume not,
        i.e. $f(c) < d$\footnote{Since $f(c) = \liminfty f(x_n) \in X$ hence $\liminfty f(x_n) \leq d$}.
        Then by problem sheet $1$
        (this term)
        Q7,
        there exists a
        (small)
        neighbourhood $(c - \delta, c + \delta) \in (a, b)$ such that $f(x) < d$ for all $x \in (c - \delta, c + \delta)$.

        In particular, $f(c + \delta / 2) < d$ so $c + \delta / 2 \in X$
        but $c < c + \delta / 2$ but $c = \sup{X}$ contradiction!

        So $f(c) =  d$.
    \end{proof}
\end{theorem}

\begin{corollary}
    $f : I \to \R$ continuous on an interval $I$.
    Then the image $f(I)$ is also an interval.
    
    \begin{proof}
        An interval $J$ is a set such that whenever $x < y \in J$,
        then all numbers in between are also in $J$.
        Now apply Intermediate Value Theorem.

        \textit{Use $x = f(a), y = f(b)$ and apply IVT.}
    \end{proof}
\end{corollary}

\begin{theorem}
    $f : [a, b] \to \R$ is continuous.
    Then $f$ takes minimum and maximum on $[a, b]$.
    
    \begin{proof}
        Only do maximum.

        Step $1$.
        
        $f$ is bounded above,
        on $[a, b]$.

        Say it did,
        then given $n \in \N$,
        exists $x_n \in [a, b]$ such that $f(x_n) > n$ by Bolzano-Weierstrass there exists a convergent subsequence $x_{n_i}$ with limit $c \in [a, b]$,
        here we use closed interval.
        So $f(n_i) \to f(c) \in \R$ with $f(n_i) > n_i$,
        at some point $n_i > c$.

        Hence $\sup\{f(a, b)\}$ exists in $\R$,
        $M = \sup\{f(a, b)\}$.
        (By term $1$)
        there exists a sequence $y_n \in f([a, b])$ such that $\liminfty y_n = M$,
        but $y_n = f(x_n)$ by continuity $\liminfty f(x_n) = M$.

        $x_n$ might not converge but by Bolzano-Weierstrass will have a converging subsequence in $[a, b]$ so $\lim x_{n_i} = c \in [a, b]$.

        Then $f(c) = f(\lim x_{n_i}) = \lim f(x_{n_i}) = \lim y_{n_i} = M$.

        Together with the Intermediate Value theorem we get the image of a continuous function on a compact interval is again a compact interval.
    \end{proof}
\end{theorem}

\begin{definition}
    Continuity on a set $X$,
    for all $c \in X$ and for all $\varepsilon > 0$,
    there exists $\delta > 0$ for all $x \in X$ with
    \[
    |x - c| < \delta \implies |f(x) - f(c)| < \varepsilon.
    \]
\end{definition}

\begin{definition}[Uniform continuity]
    $f : X \to \R$ is uniform continuous if for all $\varepsilon > 0$ there exists $\delta > 0$ such that for all $x, y \in X$ with
    \[
    |x - y| < \delta \implies |f(x) - f(y)| < \varepsilon.
    \]
    \textit{In other words}
    \[
    \forall \varepsilon > 0, \exists \delta > 0 \text{ s.t. } \forall x, y \in X, |x - y| < \delta \implies |f(x) - f(y)| < \varepsilon.
    \]
\end{definition}

\begin{theorem}
    $f : [a, b] \to \R$ continuous on a compact interval.
    Then $f$ is uniformly continuous.
    \begin{proof}
        Assume not.
        There exists $\varepsilon > 0$ such that for all $\delta > 0$ there exists $x, y \in X$ with
        \[
        |x - y| < \delta
        \]
        but
        \[
        |f(x) - f(y)| \geq \varepsilon.
        \]
        Take such a "bad" $\varepsilon > 0$.
        So for $\delta = \delta_n = \frac{1}{n}$,
        has $x_n, y_n \in [a, b]$ with $|x_n - y_n| < \delta$ but $|f(x_n) - f(y_n)| \geq \delta$.
        By Bolzano-Weierstrass for a converging subsequence $(x_{n_i})$ of the $(x_n)$
        (since $x_n \in [a, b]$)
        say $\lim x_{n_i} = x ^ {*} \in [a, b]$.

        Claim:
        also $\lim y_{n_i} = x ^ {*}$.
        Indeed,
        \begin{align*}
            |x ^ {*} - y_{n_i}| &= |x ^ {*} - x_{n_i} + x_{n_i} - y_{n_i}| \\
            &\leq |x ^ {*} - x_{n_i}| + |x ^ {*} - y_{n_i}| \\
            &\to 0 + 0 = 0
        \end{align*}
        Squeezing gives the claim.

        Claim:
        \[
        \lim f(x_{n_i}) - f(y_{n_i}) = 0.
        \]
        Indeed
        \begin{align*}
            |f(x_{n_i}) - f(y_{n_i})| &= |f(x_{n_i}) - f(x ^ {*}) + f(x ^ {*}) - f(y_{n_i})| \\
            &\leq |f(x_{n_i}) - f(x ^ {*})| + |f(x ^ {*}) - f(y_{n_i})| \\
            &\to 0 + 0 = 0
        \end{align*}
        by continuity of $f$ and $x_{n_i}, y_{n_i} \to x ^ {*}$.

        So for $n_i$ sufficiently large
        \[
        |f(x_{n_i}) - f(y_{n_i})| < \varepsilon
        \]
        contradiction!
    \end{proof}
\end{theorem}

\begin{remark}
    \begin{enumerate}[label = (\roman*)]
        \item Proof is non-constructive,
        no indication for finding the $\delta$ in general,
        hard to impossible.

        \item Statement useful/crucial for defining integration of continuous functions.

        \item In particularly nice class of functions are Lipschitz-continuous which satisfy $|f(x) - f(y)| \leq M|x - y|$ for all $x, y \in X$,
        where $M$ is the Lipschitz constant.
        These are uniform continuous.

        \item "Many" functions are Lipschitz continuous on compact intervals,
        but not all.
        For example $f(x) = \sqrt{x}$ is not Lipschitz on the interval $[0, 1]$.

        \item $f(x) = \frac{1}{x}$ on $(0, 1)$ is not uniformly continuous.

        \item $f(x) = e ^ x$ is not uniformly continuous on $[0, \infty)$.

        \item $f(x) = \log(x)$ is uniformly continuous on $[1, \infty)$.
    \end{enumerate}
\end{remark}

\subsection{Inverse functions}

\begin{theorem}
    Let $f : I \to \R$ be a continuous function on an interval $I$,
    and injective
    (1-1)
    so $f(I) = J$ is also an interval and the inverse function $f ^ {-1} : J \to I$ is also continuous.

    \begin{proof}
        One of the key steps:
        if $f$ is continuous and 1-1.
        Then $f$ is either strictly monotonically increasing or decreasing.
    \end{proof}
\end{theorem}

\newpage

\section{Differentiability}

\begin{definition}
    $f : X \to \R$
    ($X$ open).
    We say that $f$ is differentiable at a point $c \in X$ if
    \[
    \lim_{x \to c}\frac{f(x) - f(c)}{x - c}
    \]
    exists.
    If so,
    we write $f'(c)$ for the limit.
\end{definition}

\begin{lemma}[First order Taylor]\label{analy:lem:firstordertaylor}
    $f : X \to \R$,
    $f$ is differentiable at $c$ if and only if there exists a constant $n \in \R$ and a function $r(x)$ on $X$ such that
    \begin{equation}\label{analy:eq:2}
        f(x) = f(c) + m(x - c) + r(x)(x - c)
    \end{equation}
    with $r(x)$ is continuous at $c$ and $\lim_{x \to c} r(x) = r(c) = 0$.
    In that case $m = f'(c)$.
\end{lemma}

\begin{lemma}[continues = analy:lem:firstordertaylor]
    \begin{proof}
        "$\implies$":
        Set $m = f'(c)$ and
        \[
        r(x) \coloneqq \begin{cases}
            \frac{f(x) - f(c) - m(x - c)}{x - c} & x \neq c, \\
            0 & x = c.
        \end{cases}
        \]
        \eqref{analy:eq:2} holds by construction.
        Need to show
        \[
        \lim_{x \to c}r(x) = 0 = r(c),
        \]
        \begin{align*}
            \lim_{x \to c}\left(\frac{f(x) - f(c)}{x - c} - m\right) &= \lim_{x \to c}\left(\frac{f(x) - f(c)}{x - c} - f'(c)\right) = 0.
        \end{align*}

        "$\impliedby$":
        \begin{align*}
            0 &= r(c) \\
            &= \lim_{x \to c}r(x) \\
            &= \lim_{x \to c}\frac{f(x) - f(c) - m(x - c)}{x - c} \\
            &= \lim_{x \to c}\left(\frac{f(x) - f(c)}{x - c} - m\right).
        \end{align*}
        Only way this is possible if $\lim_{x \to c}\frac{f(x) - f(c)}{x - c}$ exists and is equal to $m$.
        \[
        \]
    \end{proof}
\end{lemma}

\begin{proposition}
    $f : X \to \R$ as before.
    Then if $f$ is differentiable at $x = c$,
    then $f(x)$ is also continuous at $x = c$.

    \begin{proof}
        Assume $f$ is differentiable at $x = c$.
        Then $f(x) - f(c) = (x - c) \frac{f(x) - f(c)}{x - c} \xrightarrow[x \to c]{} 0 \cdot f'(c) = 0$.
        So $\lim_{x \to c}f(x) = f(c)$,
        that is exactly continuity.
    \end{proof}
\end{proposition}

\begin{theorem}
    $f, g$ are differentiable at $x = c$.
    Then $f(x) + g(x)$ and $\alpha f(x)$ are differentiable at $x = c$.
    With
    \[
    (f + g)'(c) = f'(c) + g'(c)
    \]
    and
    \[
    (\alpha f)'(c) = \alpha f'(c).
    \]
    Also the product $f(x)g(x)$ with
    \[
    (f(x)g(x))'(c) = f'(c)g(c) + f(c)g'(c).
    \]
    Assume $f(c) \neq 0$.
    Then $\frac{1}{f(x)}$ is defined in a open neighbourhood around $x = c$ and is differentiable with
    \[
    \left(\frac{1}{f(c)}\right)' = \frac{-f'(c)}{f ^ 2(c)}
    \]
    
    \begin{proof}
        For the product.
        Write
        \[
        f(x) = f(c) + (x - c)f_1(x)
        \]
        \[
        g(x) = g(c) + (x - c)g_1(x)
        \]
        with $f_1, g_1$ continuous at $x = c$ and $\lim_{x \to c}f_1(x) = f'(c)$ and $\lim_{x \to c}g_1(x) = g'(c)$.
        Then
        \begin{align*}
            f(x)g(x) &= (f(c) + (x - c)f_1(x))(g(c) + (x - c)g_1(x)) \\
            &= f(c)g(c) + (x - c)(f_1(x)g(c) + f(c)g_1(x) + (x - c)f_1(x)g_1(x)) \\
            \intertext{with $x = c$}
            &= f'(c)g'(c) + f(c)g'(c) + 0.
        \end{align*}
        \[
        \]
    \end{proof}
\end{theorem}

\begin{theorem}[Chain rule]
    $g : X \to Y \subseteq \R$,
    $f : Y \to \R$,
    $X, Y$ open,
    $g$ is differentiable at $x = c$,
    $f$ is differentiable at $y = d = g(c)$.

    Then the composition
    \[
    f \circ g(x) : X \to \R
    \]
    is differentiable at $x = c$ and
    \[
    (f \circ g)'(c) = g'(c)f'(g(c)).
    \]

    \begin{proof}
        $g$ differentiable at $c$:
        \[
        g(x) = g(c) + g_1(x)(x - c)
        \]
        continuous at $c$ and $g_1(c) = \lim_{x \to c}g_1(x) = g'(c)$.

        $f$ differentiable at $g(c) = d$:
        \[
        f(y) = f(g(c)) + f_1(y)(y - g(c)).
        \]
        So
        \begin{align*}
            f \circ g(x) &= f(g(x)) \\
            &= f(g(c)) + f_1(g(x))(g(x) - g(c)) \\
            &= f(g(c)) + f_1(g(x))[g(c) + g_1(x)(x - c) - g(c) ^ 2] \\
            &= f(g(c)) + f_1(g(x))g_1(x)(x - c)
            \intertext{have $h(x) = f \circ g(x)$,
            $h_1(x) = f_1(g(x))g_1(x)$}
            &= h(c) + h_1(x)(x - c)
        \end{align*}
        since $f_1$ is continuous at $g(c)$ and $g_1$ is continuous at $c$,
        \begin{align*}
            \lim_{x \to c}h_1(x) &= f_1(g(c))g_1(c)
            \intertext{by continuity}
            &= f'(g(c))g'(c).
        \end{align*}
    \end{proof}
\end{theorem}

\begin{lemma}
    \[
    x \leq e ^ x - 1 \leq \frac{x}{1 - x}\qquad(x < 1).
    \]
\end{lemma}

\subsection{Inverse functions}
\begin{theorem}
    $f : I \to \R$
    ($I$ an interval)
    continuous and differentiable at $x = c$,
    and $f'(c) \neq 0$.
    Assume $f$ is $1$-$1$
    (invertible).
    So
    \[
    f ^ {-1} = g : \underset{= f(I)}{Y} \to \R
    \]
    exists and is differentiable at $y = d = f(c)$ and
    \[
    (f ^ {-1})'(d) = \frac{1}{f'(f ^ {-1}(d))} = \frac{1}{f'(c)}.
    \]

    \begin{proof}
        Simple case.
        Assume you knew that the inverse function $f ^ {-1}$ is differentiable.
        Then $f ^ {-1} \circ f(x) = x$ by the chain rule
        \[
        (f ^ {-1})'(f(x))f'(x) = 1
        \]
        \[
        (f ^ {-1}(f(x)))' = \frac{1}{f'(x)}
        \]
        write $x = f ^ {-1}(y)$,
        \[
        (f ^ {-1})'(y) = \frac{1}{f'(f ^ {-1}(y))}.
        \]
    \end{proof}
\end{theorem}

\subsection{Mean Value Theorems}

\begin{proposition}
    If $f$ is differentiable at $c$ and it has a local maximum or a local minimum at $C$,
    then $f'(c) = 0$.

    \begin{proof}
        $f'(c) = \lim_{x \to c}\frac{f(x) - f(c)}{x - c}$.
        If $x > c$,
        but $x$ is near $c$,
        then $f(x) \leq f(c)$ since $c$ is a local maximum.
        In particular $\frac{f(x) - f(c)}{x - c} \leq 0$.
        Similarly,
        for $x < c$,
        $\frac{f(x) - f(c)}{x - c} \geq 0$ so $\lim_{x \to c ^ {+}}\frac{f(x) - f(c)}{x - c} \leq 0, \lim_{x \to c ^ {-}}\frac{f(x) - f(c)}{x - c} \geq 0 \implies f'(c) = 0$.
    \end{proof}
\end{proposition}

\begin{theorem}[Rolle's Theorem]
    Let $f : [a, b] \to \R$ be continuous and differentiable on $(a, b)$,
    and suppose $f(a) = f(b)$.
    Then there exists $c \in (a, b)$ with $f'(c) = 0$.

    \begin{proof}
        A continuous function on a closed interval attains a maximum and a minimum.
        So there is a $c \in [a, b]$ with $f(c) \geq f(x)$ for all $x \in [a, b]$,
        $d \in [a, b]$ with $f(d) \leq f(x)$ for all $x \in [a, b]$.
        If $c \in (a, b)$,
        we get $f'(c) = 0$ by last result.
        If $c = a$ or $c = b$.
        Then look at minimum $d$ if $f \in (a, b)$ we can use the last result again $f'(d) = 0$.
        If $d = a$ or $d = b$,
        then $f(d) = f(c)$ and the whole function is constant.
        Then $f'(x) = 0$ for all $x \in (a, b)$.
    \end{proof}
\end{theorem}

\begin{theorem}[Mean Value Theorem]
    Let $f : [a, b] \to \R$ be continuous and differentiable on $(a, b)$.
    Then there exists a $c \in (a, b)$ such that $f'(c) = \frac{f(b) - f(a)}{b - a}$.

    \begin{proof}
        $g(x) = f(x) - \frac{f(b) - f(a)}{b - a}(x - a)$.
        Then $g$ is continuous on $[a, b]$ and differentiable on $(a, b)$.
        \[
        g(b) = f(b) - \frac{f(b) - f(a)}{b - a}(b - a) = f(b) - f(b) + f(a) = f(a).
        \]
        \[
        g(a) = f(a).
        \]
        By Rolle,
        there is a $c \in (a, b)$ with $g'(c) = 0$.
        \[
        g'(c) = f'(c) - \frac{f(b) - f(a)}{b - a} = 0.
        \]
    \end{proof}
\end{theorem}

\begin{theorem}
    Let $f : I \to \R$ be continuous on an interval $I$,
    differentiable in its interior.
    \begin{enumerate}[label = (\roman*)]
        \item If $f'(x) = 0$ for all $x$,
        then $f$ is constant.

        \item If $f'(x) \geq 0$
        ($\leq 0$)
        for all $x$,
        then $f$ is monotonically increasing
        (decreasing).

        \item If $f'(x) > 0$
        ($< 0$)
        for all $x$,
        then $f$ is strictly monotonically increasing
        (decreasing).
    \end{enumerate}
    \begin{proof}
        Let $c < d$ be two points in $I$.
        By MVT there is an $\alpha \in (c, d)$ such that
        \[
        f(d) - f(c) = (d - c)f'(\alpha) = \begin{dcases*}
            0 & in case (i) \\
            \geq 0 & in case (ii) \\
            > 0 & in case (iii).
        \end{dcases*}
        \]
        In case (i) $f(d) = f(c)$,
        in case (ii) $f(d) \geq f(c)$,
        in case (iii) $f(d) > f(c)$.
    \end{proof}
\end{theorem}

\begin{theorem}[Cauchy's Generalised Mean Value Theorem]
    Let $f, g : [a, b] \to \R$ continuous and differentiable on $(a, b)$.
    Assume $g'(x) \neq 0$ for all $x \in (a, b)$.
    Then there exists $c \in (a, b)$ such that
    \[
    \frac{f'(c)}{g'(c)} = \frac{f(b) - f(a)}{g(b) - g(a)}.
    \]

    \begin{proof}
        Consider
        \[
        h(x) = (g(b) - g(a))f(x) - (f(b) - f(a))g(x)
        \]
        continuous on $[a, b]$,
        differentiable on $(a, b)$.
        By Rolle there is $c \in (a, b)$ with $h'(c) = 0$
        \[
        h'(c) = (g(b) - g(a))f'(c) - (f(b) - f(a))g'(c) = 0.
        \]
    \end{proof}
\end{theorem}

\subsection{L'H\^opital's Rule}

\begin{theorem}
    Let $f$ and $g$ be two differentiable functions on $(a, b)$.
    Assume that
    \[
    \lim_{x \to a ^ {+}}f(x) = 0
    \]
    and
    \[
    \lim_{x \to a ^ {+}}g(x) = 0
    \]
    and $g(x) \neq 0$,
    $g'(x) \neq 0$ for all $x$ on $(a, b)$.
    Then
    If
    \[
    \lim_{x \to a ^ {+}}\frac{f'(x)}{g'(x)}
    \]
    exists then also
    \[
    \lim_{x \to a ^ {+}}\frac{f(x)}{g(x)}
    \]
    exists
    and
    \[
    \lim_{x \to a ^ {+}}\frac{f(x)}{g(x)} = \lim_{x \to a ^ {+}}\frac{f'(x)}{g'(x)}.
    \]
    
    \begin{proof}
        We can extend $f, g$ continuously to $x = a$ by setting $f(a) = g(a) = 0$.
        Take any  sequence $x_n \in (a, b)$ with $\liminfty x_n = a$.

        Need to show
        \[
        \liminfty \frac{f(x_n)}{g(x_n)} = L.
        \]
        Apply the generalised mean value theorem for $f$ and $g$ on the intervals $[a, x_n]$.
        So exists a $y_n \in (a, x_n)$ such that $\frac{f'(y_n)}{g'(y_n)} = \frac{f(x_n) - f(a)}{g(x_n) - g(a)} = \frac{f(x_n)}{g(x_n)}$.

        By squeezing $\liminfty y_n = a$
        ($a < y_n < \underbrace{x_n}_{\to a}$)
        so
        \[
        L = \liminfty\frac{f(y_n)}{g(y_n)} = \liminfty\frac{f(x_n)}{g(x_n)}.
        \]
    \end{proof}
\end{theorem}

\begin{remark}
    \begin{enumerate}[label = (\roman*)]
        \item Also holds for left-sided limits or both sided limits.

        \item $a = \pm\infty$ is also ok.

        \item "$\frac{\infty}{\infty}$" is fine.

        \item "$\infty \cdot 0$" can often be handled by $f(x)g(x) = \frac{f(x)}{\frac{1}{g(x)}}$.

        \item $\lim_{x \to 0}\frac{e ^ x - 1}{x} = \lim_{x \to 0}\frac{e ^ x}{1} = 1$.

        \item $\lim_{x \to 1}\frac{x ^ 2 + x + 2}{x - 2} = \lim_{x \to 1}\frac{2x + 1}{-2} = 2$ but this is wrong,
        we get $\frac{4}{1 - 2} = -4$.

        \item Powers beat $\log$.
        
        $a > 0$:
        \[
        \liminfty[x]\frac{\log(x)}{x ^ a} = \liminfty[x]\frac{\frac{1}{x}}{a \cdot x ^ {a - 1}} = \liminfty[x]\frac{1}{ax ^ a} = 0
        \]
        \[
        \lim_{x \to 0 ^ {+}}x ^ a\log(x) = \lim_{x \to 0 ^ {+}}\frac{\log(x)}{x ^ {-a}} = \lim_{x \to 0 ^ {+}}\frac{\frac{1}{x}}{-ax ^ {-a - 1}} = \lim_{x \to 0 ^ {+}}-\frac{1}{a}x ^ a = 0.
        \]

        \item Exponentials beat powers.
        
        $a > 0$:
        \begin{align*}
            \liminfty[x]\frac{x ^ a}{e ^ x} &= \liminfty[x]\frac{ax ^ {a - 1}}{e ^ x} \\
            &= \liminfty[x]\frac{a(a - 1)x ^ {a - 2}}{e ^ x} &= \\
            \dotsi \\
            &= \frac{(a(a - 1)(a - 2)\dotsi 3 \cdot 2 \cdot 1) \cdot x ^ {-1}}{e ^ x} = 0.
        \end{align*}

        \item
        \[
        \lim_{x \to 0}\frac{\log(1 + x) - x}{x ^ 2} = \lim_{x \to 0}\frac{\frac{1}{1 + x} - 1}{2x} = \lim_{x \to 0}\frac{1 - (1 + x)}{2x(1 + x)} = \lim_{x \to 0}\frac{-\frac{1}{2}}{1 + x} = -\frac{1}{2}.
        \]
        So $\frac{\log(1 + x ^ 2) - x}{x ^ 2} = -\frac{1}{2} + r(x)$ with $\lim_{x \to 0}r(x) = 0$.

        So $\log(1 + x ^ 2) = \underbrace{1}_{\frac{d}{dx}\log(1 + x)\text{ at x = 1}}x - \frac{1}{2}x ^ 2 + r(x)x ^ 2$.
    \end{enumerate}
\end{remark}

\subsection{Taylor}

\begin{theorem}[Taylor's Theorem
(Peano remainder)]
    $f : I \to \R$ $n$-times differentiable.
    Then there exists a function $r_n(x)$ with $\lim_{x \to c}r_n(x) = 0$ such that
    \begin{equation}\label{analy:eq:3}
        f(x) = T_{f, c} ^ {(n)}(x) + r_n(x)(x - c) ^ n.
    \end{equation}

    \begin{proof}
        Solve for $r_n(x)$ in \eqref{analy:eq:3}.
        \[
        r_n(x) = \frac{f(x) - T_{f, c} ^{(n)}(x)}{(x - c) ^ n}.
        \]
        Need to compute $\lim_{x \to c}r_n(x)$.
        Apply L'H\^opital $n$-times get $\lim_{x \to c}\frac{f ^ {(n)}(x) - f ^ {(n)}(c)}{n!} = 0$.
    \end{proof}
\end{theorem}

\begin{theorem}[Taylor's Theorem
(Lagrange remainder)]
    Assume in addition that $f$ is $(n + 1)$ times differentiable.
    Then there exists a $\xi$ between $x$ and $c$ such that
    \[
    f(x) = T_{f, c} ^ {(n)}(x) + \frac{f ^ {(n + 1)}(\xi)}{(n + 1)!}(x - c) ^ {n + 1}.
    \]

    \begin{proof}
        Fix $x \in I$.
        Define
        \[
        F(t) = f(x) - T_{f, t} ^ {(n)}(x)
        \]
        \[
        f(x) - \left[f(t) - f'(t)(x - t) + \dotsc + \frac{f ^ {(n)}(t)}{n!}(x - t) ^ n\right].
        \]
        So $F(c) = r_n(x)(x - c) ^ n$.
        Have $F'(t) = -\frac{f ^ {(n + 1)}(t)}{n!}(x - t) ^ n$.
        Apply Cauchy's generalised mean value theorem for $F(t)$ and $G(t) = (x - t) ^ {n + 1}$.
        Then
        \[
        \frac{r_n(x)}{x - c} = \frac{F(c)}{(x - c) ^ {n + 1}} = \frac{F(c)}{G(c)} = \frac{F(c) - \overbrace{F(x)}^{= 0}}{G(c) - \underbrace{G(x)}_{= 0}} = \frac{F'(\xi)}{G'(\xi)} = \frac{{-\frac{f ^ {(n + 1)}(\xi)}{n!}(x - \xi) ^ n}}{-(n + 1)(x - \xi) ^ n} = \frac{f ^ {(n + 1)}(\xi)}{(n + 1)!}
        \]
        with $\xi$ between $x$ and $c$.
    \end{proof}
\end{theorem}

\newpage

\section{Power Series}

\begin{definition}
    \[
    \infsumo a_kx ^ k
    \]
    is a power series.
    $a_k \in \R$ and $x \in \R$.
\end{definition}

\begin{theorem}[Cauchy-Hadamard]
    Given $\infsumo a_kx ^ k$,
    there exists $R \geq 0$
    (possibly $\infty$)
    ($R$ is the radius of convergence)
    such that
    \begin{enumerate}[label = (\roman*)]
        \item $\infsumo a_kx ^ k$ converges absolutely for $|x| < R$,

        \item $\infsumo a_kx ^ k$ diverges for $|x| > R$.
    \end{enumerate}
    In fact.
    If $c = \limsup\sqrt[k]{|a_k|}$ then $R = \frac{1}{c}$
    (if $c = 0, \infty$,
    $R = \infty, 0$).
    
    \begin{proof}
        We apply the root test to
        \[
        \infsumo a_kx ^ k
        \]
        \begin{align*}
            \limsup\sqrt[k]{|a_kx ^ k|} &= \limsup\sqrt[k]{|a_k|}|x| \\
            &= |x|\limsup\sqrt[k]{|a_k|} \\
            &= c|x| &= \begin{cases}
                < 1 & \text{if } |x| < 1 / c \\
                > 1 & \text{if } |x| > 1 / c
            \end{cases}
        \end{align*}
        with absolute convergence in the first case and divergence in the second.
    \end{proof}
\end{theorem}

\begin{remark}
    \begin{enumerate}[label = (\roman*)]
        \item In practice,
        the ratio test works just as well:
        \[
        \frac{|a_{k + 1}|x| ^ {k + 1}}{|a_k||x| ^ k} = \frac{|a_{k + 1}|}{|a_k|}|x|
        \]
        if $\liminfty[k]\frac{|a_{k + 1}|}{|a_k|}$ exists and is equal to $c$
        (say)
        then as before $R = \frac{1}{c}$.

        \item $x = \pm R$ endpoints of interval of convergence anything can happen.
        To check do not use the ratio or root test.
        Plug in $x = \pm R$,
        and see what happens.
    \end{enumerate}
\end{remark}

\begin{corollary}
    If $\infsumo a_kc ^ k$ converges for some $c \in \R$,
    then $\sum a_kx ^ k$ converges for all $x \in (-|c|, |c|)$.

    \begin{proof}
        Must have $|c| \leq R \implies (-|c|, |c|) \subseteq (-R, R)$.
    \end{proof}
\end{corollary}

\begin{proposition}\label{analy:prop:powserderivandantiderivsameradconv}
    $\infsumo a_kx ^ k$ with radius $R > 0$.
    Then the formal derivative
    \[
    \infsum ka_kx ^ {k - 1} = \frac{1}{x}\infsum ka_kx ^ k
    \]
    anti-derivative
    \[
    \infsumo \frac{1}{k + 1}a_kx ^ {k + 1}
    \]
    have the same radius of convergence $R$.
    
    \begin{proof}
        \begin{align*}
            \limsup\sqrt[k]{k|a_k|} &= \lim\sqrt[k]{k}\limsup\sqrt[k]{|a_k|} \\
            &= 1 \cdot \limsup\sqrt[k]{|a_k|}.
        \end{align*}
        Anti-derivative the same.
    \end{proof}
\end{proposition}

\subsection{Power series as a function}

\begin{theorem}
    Power series are continuous in $(-R, R)$.
    
    In fact,
    they are "locally" Lipschitz,
    that is,
    for any $0 < r < \R$ there exists a constant $M = M_r$ such that
    \[
    |f(x) - f(y)| \leq M_r|x - y|
    \]
    for all $x \in [-r, r]$.

    \begin{proof}
        \begin{align*}
            0 &\leq |f_n(x) - f_n(y)| \\
            &= \left|\sum_{k = 1}^{n}a_k(x ^ k - y ^ k)\right| \\
            &= |x - y|\cdot\left|\sum_{k = 1}^{n}a_k(x ^ {k - 1} + x ^ {k - 2}y + \dotsc + xy ^ {k - 2} + y ^ {k - 1})\right|
            \intertext{assume $|x|, |y| \leq r$}
            &\leq |x - y|\cdot\sum_{k = 1}^{n}|a_k|\cdot k \cdot r ^ {k - 1} \\
            &\xrightarrow[n \to \infty]{} |x - y|\cdot\sum_{k = 1}^{n}\underbrace{|a_k|\cdot k \cdot r ^ {k - 1}}_{= M_r}
        \end{align*}
        converges by \autoref{analy:prop:powserderivandantiderivsameradconv} which implies by squeezing
        \[
        |f(x) - f(y)| \leq |x - y|M_r.
        \]
    \end{proof}
\end{theorem}

\begin{theorem}\label{analy:thm:powerserisdiff}
    The power series $f(x) = \infsumo a_kx ^ k$ is differentiable in $(-R, R)$ with term-wise derivative
    \[
    f'(x) = \infsum ka_kx ^ {k - 1}
    \]
    and $f ^ {(n)}(0) = n!a_n$.

    \begin{proof}
        Next section.
        
        Assuming the term-wise derivative,
        we have
        \begin{align*}
            f'(0) &= \infsum ka_k0 ^ {k - 1} \underset{k = 0}{=} 1 \cdot a_1 \\
            f''(0) &= \infsum[k = 2]k(k - 1)a_kx ^ {k - 2} = 2(2 - 1)a_2 = 2 \cdot a_2 \\
            &\vdots \\
            f ^ {(n)}(0) &= \infsum[k = n]k(k - 1) \dotsi (k - n + 1)a_kx ^ {k - n} \underset{x = 0}{=} n!a_n.
        \end{align*}
    \end{proof}
\end{theorem}

\begin{theorem}[Identity Theorem for Power Series]
    Assume
    \[
    \infsumo a_kx ^ k = \infsumo b_kx ^ k
    \]
    in some
    (small)
    neighbourhood of $x = 0$.
    Then $a_k = b_k$.

    \begin{proof}
        Call $f(x) = \infsumo a_kx ^ k$,
        then $n!b_n = f ^ {(n)}(0) = n!a_n$ by \autoref{analy:thm:powerserisdiff}.
        So $a_n = b_n$ for all $n$.
    \end{proof}
\end{theorem}

\begin{theorem}
    \begin{enumerate}[label = (\roman*)]
        \item $\exp(0) = 1$.

        \item The exponential function is infinitely often differentiable on $\R$ with
        \[
        \exp'(x) = \exp(x).
        \]

        \item For all $x, y \in \R$ we have
        \begin{align*}
            \exp(x + y) &= \exp(x)\exp(y); \\
            \exp(-x) &= \frac{1}{\exp(x)}.
        \end{align*}

        \item $\exp(x) > 0$ for all $x \in \R$.

        \item $\exp(x)$ is strictly monotone increasing.

        \item
        \[
        \liminfty[x]\exp(x) = \infty\qquad\text{and}\qquad\lim_{x \to -\infty}\exp(x) = 0.
        \]
    \end{enumerate}

    \begin{proof}\phantom{}
        \begin{enumerate}[label = (\roman*)]
            \item Trivial by substitution.

            \item
            \begin{align*}
                \left(\infsumo \frac{x ^ k}{k!}\right)' &= \infsum\frac{kx ^ {k - 1}}{k!} \\
                &= \infsumo\frac{(k + 1)x ^ k}{(k + 1)!} \\
                &= \infsumo\frac{x ^ k}{k!}.
            \end{align*}

            \item Two options:
            for the first property we can use the Cauchy product for infinite series.

            Set $f(t) = \exp(x + t)\cdot\exp(y - t)$.
            \[
            f(0) = \exp(x)\exp(y)
            \]
            \[
            f(y) = \exp(x + y) \cdot 1 = \exp(x + y)
            \]
            \[
            f'(t) = \exp(x + t)\exp(y - t) + \exp(x + t)(-1)\exp(y - t) = 0
            \]
            by the mean value theorem $f(t)$ is constant.

            For $\exp(-x) = \frac{1}{\exp(x)}$.
            Plug in $y = -x$.

            \item True for $x > 0$,
            for $x < 0$ use $\exp(x) = \frac{1}{\exp(-x)} > 0$.

            \item $\exp'(x) = \exp(x) > 0$ hence is strictly increasing.

            \item For $x > 0$ $\exp(x) > 1 + x$ $1 + x$ is unbounded so $\exp(x)$ is unbounded.
            Then for $x < 0$ reuse $\exp(x) = \frac{1}{\exp(-x)}$.
        \end{enumerate}
    \end{proof}
\end{theorem}

\begin{definition}
    We define the sine and cosine function by
    \[
    \sin(x) \coloneqq \infsumo\frac{(-1) ^ k}{(2k + 1)!}x ^ {2k + 1};\qquad\cos(x) \coloneqq \infsumo\frac{(-1) ^ k}{(2k)!}x ^ {2k}.
    \]
\end{definition}

\begin{theorem}
    We have
    \begin{enumerate}[label = (\roman*)]
        \item $\sin(0) = 0$ and $\cos(0) = 1$.

        \item The sine-function is odd;
        cosine is even.

        \item Sine and cosine are infinitely often differentiable on $\R$ with
        \[
        \sin'(x) = \cos(x)\qquad\text{and}\qquad\cos'(x) = -\sin(x).
        \]

        \item For all $x, y \in \R$ we have
        \begin{align*}
            \sin(x + y) &= \sin(x)\cos(y) + \cos(x)\sin(y) \\
            \cos(x + y) &= \cos(x)\cos(y) + \sin(x)\sin(y)
        \end{align*}

        \item For all $x \in \R$ we have
        \[
        \sin ^ 2(x) + \cos ^ 2(x) = 1.
        \]
        In particular,
        $|\sin(x)| \leq 1$;
        $|\cos(x)| \leq 1$ for all $x \in \R$.
    \end{enumerate}

    \begin{proof}\phantom{}
        \begin{enumerate}[label = (\roman*)]
            \item By definition.
            
            \item By definition.

            \item Differentiating term-wise.

            \item
            \[
            f(t) \coloneqq \sin(x + t)\cos(y - t) + \cos(x + t)\sin(y - t).
            \]
            \begin{align*}
                f(0) &= \sin(x)\cos(y) + \cos(x)\sin(y) \\
                f(y) &= \sin(x + y)\cos(0) + \cos(x + y)\sin(0) = \sin(x + y) \\
                f'(t) &= \dotsi = 0.
            \end{align*}
            Hence $f(t)$ is constant so $f(0) = f(y)$,
            addition law for sine.

            \item
            \[
            \sin ^ 2(x) + \cos ^ 2(x) = 1
            \]
            true for $x = 0$.
            Differentiate
            \[
            2\sin(x)\cos(x) + 2\cos(x)(-\sin(x)) = 0
            \]
            by mean value theorem.
        \end{enumerate}
    \end{proof}
\end{theorem}

\begin{theorem}
    The equation $\cos(x) = 0$ has a smallest positive solution.

    \begin{proof}
        \[
        \cos(2) = \underbrace{1 - 2 + \frac{16}{24}}_{-\frac{2}{3}} + \underbrace{\infsum[k = 3]\frac{(-1) ^ k}{(2k)!}2 ^ {2k}}_{-+-\dotsc} < 0
        \]
        by alternating series test.
        \[
        \cos(0) = 1 > 0
        \]
        by IVT there exists a zero at $\frac{\pi}{2}$.
    \end{proof}
\end{theorem}

\begin{definition}
    We denote twice the smallest positive root of cosine by $\pi$,
    that is,
    \[
    \cos\left(\frac{\pi}{2}\right) = 0.
    \]
\end{definition}

\begin{theorem}
    \[
    \sin\left(\frac{\pi}{2}\right) = 1.
    \]
\end{theorem}

\begin{remark}
    \begin{enumerate}[label = (\roman*)]
        \item Radius of convergence $R \geq 0$.

        \item We can term-wise differentiate or take antiderivatives.

        \item $f ^ {(k)}(c) = k!a_k$
    \end{enumerate}
\end{remark}

\begin{proposition}
    Assume $f(x) = \infsumo a_kx ^ k$ is given by a power series converging in $(-R, R)$,
    ($R > 0$).

    Let $c \in (-R, R)$.
    Then $T_{f, c}(x)$ converges with radius of convergence
    \[
    ||c| - R|
    \]
    and is equal to $f(x)$.

    \begin{proof}
        Skipped.
        More naturally explained in the context of complex analysis.
    \end{proof}
\end{proposition}

\newpage

\section{Sequences of Functions}

\begin{definition}[Pointwise limit]
    We say $f(x) : I \to \R$ is the pointwise limit of $(f_n(x))$ if for all $x \in \R$ we have $\liminfty f_n(x) = f(x)$.
    I,e,
    \[
    \forall x \in I, \forall \varepsilon > 0, \exists N > 0, \forall n \geq N, |f_n(x) - f(x)| < \varepsilon.
    \]
\end{definition}

\begin{definition}[Uniform convergence]
    $f_n \to f$ uniformly on $I$.
    \[
    \forall \varepsilon > 0, \exists N > 0, \forall x \in I, \forall n \geq N, |f_n(x) - f(x)| < \varepsilon.
    \]
\end{definition}

\begin{definition}[Uniform Convergence on Compact Subsets Subintervals.]
    $f_n : I \to \R$,
    $f_n \to f$ uniformly in compact subintervals.
    If $f_n \to f$ uniformly on all $[a, b] \subset I$.
\end{definition}

\begin{lemma}[Criterion]
    \begin{enumerate}[label = (\roman*)]
        \item If $|f(x) - f_n(x)| \leq a_n$ for all $x \in I$ with $\liminfty a_n = 0$ then we have uniform convergence on $I$.

        \item if there exists $\varepsilon > 0$ and a sequence $x_n \in I$ such that for all $n$ sufficiently large.
        \[
        |f(x_n) - f_n(x_n)| \geq \varepsilon
        \]
        then convergence is not uniform on $I$.
    \end{enumerate}
\end{lemma}

\subsection{Uniform Convergence and Continuity}

\begin{theorem}
    Assume $f_n \to f$ uniformly on $I$,
    continuous on $I$.
    Then $f(x)$ is continuous on $I$.

    \begin{proof}
        ($\frac{\varepsilon}{3}$-trick).

        Have $f_n \to f$ uniformly,
        i.e. there exists an $N$ such that for all $n \geq N$ and for all $x \in I$,
        \[
        |f(x) - f_n(x)| < \varepsilon.
        \]
        Take that $N$,
        then $f_N(x)$ is continuous.
        Given $c \in I$.
        Find $\delta > 0$,
        such that $|f_N(x) - f_N(c)| < \varepsilon$ for all $x$ with $|x - c| < \delta$
        \begin{align*}
            |f(x) - f(c)| &= |f(x) - f_N(x) + f_N(x) - f_N(c) + f_N(c) - f(c)| \\
            &\leq |f(x) - f_N(x)| + |f_N(x) - f_N(c)| + |f_N(c) - f(c)| &(\text{for $|x - c| < \delta$})\\
            &< \varepsilon + \varepsilon + \varepsilon \\
            &= 3\varepsilon.
        \end{align*}

        \textit{Note:
        could change all $\varepsilon$ to $\varepsilon / 3$ to get $<\varepsilon$.}
    \end{proof}
\end{theorem}

\begin{theorem}
    $f_n : I \to \R$ continuous,
    $f_n \to f$ uniformly on compact subsets of $I$.
    Then $f(x)$ is continuous.

    \begin{proof}
        Take $c \in I$,
        find an $I$ interval $[a, b]$ containing $c$.
        One $[a, b]$ we have uniform convergence which implies continuity of $f$ in $[a, b] \ni c$.
    \end{proof}
\end{theorem}

\begin{theorem}
    $f_n : I \to \R$ differentiable with $f_n'$ continuous.
    Assume $f_n \to f$ pointwise,
    $f_n'$ converges uniformly
    (in compact subsets)
    to $g(x)$
    (for some function $g(x)$).

    Then $f(x)$ is differentiable on $I$ with $f'(x) = g(x) = \liminfty f_n'(x)$
\end{theorem}

\begin{theorem}[Weierstrass $M$-test]
    $\infsumo f_k(x)$ on $I$.
    Assume
    \[
    |f_k(x)| \leq M_k
    \]
    independent of $x$.
    \[
    \infsumo M_k < \infty
    \]
    converges.
    Then $\infsumo f_k(x)$ uniformly on $I$.

    \begin{proof}
        By comparison
        \[
        \sum|f_k(x)| \leq \sum M_k < \infty
        \]
        so $\sum f_k(x)$ converges absolutely to a function $f(x)$.

        Set $L = \infsumo M_k = \liminfty\left(\sum_{k = 0}^{n}M_k\right)$.
        So given $\varepsilon > 0$,
        there exists an $N$ such that
        \[
        \left|L - \sum_{k = 0}^{n}M_k\right| = \infsum[k = n + 1]M_k < \varepsilon
        \]
        for all $n \geq N$.

        Will show
        \[
        \left|f(x) - \infsumo f_k(x)\right| < \varepsilon.
        \]
        for all $n \geq N$.

        Indeed:
        \[
        \left|f(x) - \sum_{k = 0}^{n}f_k(x)\right| = \left|\infsum[k = n + 1]f_k(x)\right| \leq \infsum[k = n + 1]|f_k(x)| \leq \infsum[k = n + 1]M_k < \varepsilon.
        \]
    \end{proof}
\end{theorem}

\begin{theorem}
    Power series $\infsumo a_kx ^ k$ are continuous in $(-R, R)$.

    Since they converge uniformly on compact subsets of $(-R, R)$.

    \begin{proof}
        Take $[a, b] \subset [-r, r] \subset (-R, R)$,
        $r \in \R$.

        Take $x \in [-r, r]$.
        Then
        \[
        \infsumo|\underbrace{a_kx ^ k}_{= f_k(x)}| \leq \sum\underbrace{|a_k|r ^ k}_{= M_k} < \infty
        \]
        since $r < R$ by $M$-test and the previous theorem.
    \end{proof}
\end{theorem}

\newpage

\section{Regulated Functions}

\begin{definition}[Step function]
    $f : [a, b] \to \R$.
    Exists a partition
    \[
    x_0 = a < x_1 < x_2 < \dotsc < x_N = b
    \]
    of $[a, b]$ such that on each open subinterval $(x_k, x_{k + 1})$ the function $f(x)$ is constant.
\end{definition}

\begin{proposition}
    Let $f, g$ be step functions.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item $af(x) + bg(x)$ is a step function.

        \item $f(x)g(x)$ is a step function.

        \item $|f(x)|$ is a step function.

        \item $\max(f, g) = \frac{1}{2}(f + g) + \frac{1}{2}|f - g|$ is a step function.

        \item $\min(f, g)$ is a step function.
    \end{enumerate}
\end{proposition}

\begin{definition}[Regulated functions]
    $f : [a, b] \to \R$ is called regulated if there exists a sequence $f_n(x) : [a, b] \to \R$ of step functions such that $f_n \to f$ uniformly.
\end{definition}

\begin{lemma}[Sequence-free characterisation]
    $f$ is regulated if and only if,
    for all $\varepsilon > 0$ there exists a step function $g(x)$ such that
    \[
    |f(x) - g(x)| < \varepsilon
    \]
    for all $x \in [a, b]$.

    \begin{proof}
        \textit{Note:
        by the definition of a regulated function we need uniform convergence.}
        
        Uniform convergence:
        \[
        \forall \varepsilon > 0, \exists N, \forall n \geq N, \forall x \in [a, b], |f(x) - f_N(x)| < \varepsilon.
        \]
        
        "$\implies$".
        Given $\varepsilon > 0$.
        Take $N$.
        Then $|f_N(x) - f(x)| < \varepsilon$ for all $x$.

        "$\impliedby$".
        Take $\varepsilon = 1 / n$.
        Find by hypothesis a step function $f_n(x)$ with $|f(x) - f_n(x)| < \frac{1}{n}$,
        for all $x \in [a, b]$.
        Since $\liminfty\frac{1}{n} = 0$ hence $f_n \to f$ uniformly.
    \end{proof}
\end{lemma}

\begin{theorem}
    Continuous functions on $[a, b]$ are regulated.

    \begin{proof}
        We will use that $f(x)$ is uniformly continuous on $[a, b]$.
        \begin{equation}\label{analy:eq:4}\tag{*}
            \forall \varepsilon > 0, \exists \delta > 0, \forall x, y \in [a, b], |x - y| < \delta \implies |f(x) - f(y)| < \varepsilon.
        \end{equation}

        Given $\varepsilon > 0$,
        need a step function $g(x)$ on $[a, b]$ such that $|f(x) - g(x)| < \varepsilon$ for all $x \in [a, b]$.
        We have $\delta > 0$ such that \eqref{analy:eq:4} holds.
        Take a partition $a = x_0, x_1, x_2, \dotsc x_{N + 1}, x_N = b$ such that
        \[
        |x_{k + 1} - x_k| < \delta.
        \]
        To define the step function $g(x)$,
        take any point $x_k ^ {*} \in [x_k, x_{k + 1}]$,
        then
        \[
        g(x) = \begin{cases}
            f(x_k ^ {*}), &\text{if } x \in [x_k, x_{k + 1}) \\
            f(b), &\text{if } x = b = x_N.
        \end{cases}
        \]
        Then take $x \in [a, b]$,
        so $x$ lies in one of the subintervals,
        say $x \in [x_k, x_{k + 1})$.
        Then
        \begin{align*}
            |f(x) - g(x)| &= |f(x) - f(x_k ^ {*})| &(\text{$x, x_k ^ {*} \in [x_k, x_{k + 1})$}) \\
            &< \varepsilon &(\text{since $|x - x_k ^ {*}| < \delta$}).
        \end{align*}
    \end{proof}
\end{theorem}

\begin{lemma}
    $f_n \to f$ uniformly,
    $f_n$ step functions.
    Then $f_n(x)$,
    $f(x)$ are uniformly bounded.
    I.e. $|f_n(x)|, |f(x)| < C$ for all $x \in [a, b]$.

    \begin{proof}
        Take $\varepsilon = 1$.
        Exists $N$ such that $|f_n(x) - f(x)| < \varepsilon = 1$ for all $x \in [a, b]$ and for all $n \geq N$.
        So $|f(x) - f_N(x)| < \varepsilon = 1$ for all $x$,
        so $|f(x)| < |f_N(x)| + 1$,
        $f_N(x)$ bounded since $f_N(x)$ takes only finitely many values.
        Same for $f_n(x)$,
        $n \geq N$.
        $f_1, \dotsc, f_{N - 1}$ are universally bounded.
    \end{proof}
\end{lemma}

\begin{proposition}[Regulated COLT]
    $f_n \to f$,
    $g_n \to g$,
    $f, g$ regulated.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item $af + bg$ regulated.

        \item $fg$ regulated.

        \item $|f|$ regulated.
        
        \item $\max\{f, g\}$ regulated.
        
        \item $\min\{f, g\}$ regulated.
    \end{enumerate}

    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
            \item $af_n + bg_n \to af + bg$.

            \item $f_ng_n \to fg$.

            \item $|f_n| \to f$.
        \end{enumerate}
    \end{proof}
\end{proposition}

\newpage

\section{Integration}

\begin{definition}[Regulated integral]
    $f : [a, b] \to \R$ regulated,
    say $f_n \to f$,
    $f_n$ step functions.
    Define the integral by
    \[
    I(f) = \liminfty I(f_n).
    \]
\end{definition}

\subsection{Properties of the integral}

\begin{theorem}[Properties of the integral]\phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item Linearity
        \begin{align*}
            \int_{a}^{b}f(t) + g(t)\,dt &= \int_{a}^{b}f(t)\,dt + \int_{a}^{b}g(t)\,dt. \\
            c\int_{a}^{b}f(t)\,dt &= c\int_{a}^{b}f(t)\,dt.
        \end{align*}

        \item Monotonicity

        \begin{itemize}
            \item 
            if $f(t) \leq g(t)$ for all $t \in [a, b]$ then
            \[
            \int_{a}^{b}f(t)\,dt \leq \int_{a}^{b}g(t)\,dt.
            \]

            \item
            In particular,
            if $m = \inf_{t \in [a, b]}{f(t)}$,
            $M = \sup_{t \in [a, b]}{f(t)}$ then
            \[
            m(b - a) \leq \int_{a}^{b}f(t)\,dt \leq M(b - a).
            \]

            \item
            \[
            \left|\int_{a}^{b}f(t)\,dt\right| \leq \int_{a}^{b}|f(t)|\,dt.
            \]
        \end{itemize}
        \item Additivity
        \[
        \int_{a}^{b}f(t)\,dt + \int_{b}^{c}f(t)\,dt = \int_{a}^{c}f(t)\,dt.
        \]
    \end{enumerate}

    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
            \item \phantom{}
            
            \item
            By linearity,
            it is enough to consider $f(t) \geq 0$,
            and show $\int_{a}^{b}f(t)\,dt \geq 0$.
            (Consider $g(t) - f(t)$).

            So take $f_n \to f$ uniformly.
            Given $\varepsilon > 0$,
            find $f_n(x)$ such that $|f(x) - f_n(x)| < \varepsilon$ for all $x \in [a, b]$.
            By $f(t) \geq 0$,
            we get $f_n(x) > -\varepsilon$ for all $x \in [a, b]$.
            This implies $I(f_n) > -\varepsilon(b - a)$ this implies $\liminfty I(f_n)$ is bigger than any negative number,
            shows that $I(f) \geq 0$.

            $|f(t)| = \pm f(t)$,
            $f(t) \leq |f(t)|$.
        \end{enumerate}
    \end{proof}
\end{theorem}

\begin{theorem}[Mean Value Theorem for Intervals]
    Let $f(x)$ be continuous on $[a, b]$.
    Then there exists $c \in [a, b]$ such that
    \[
    \int_{a}^{b}f(t)\,dt = f(c)(b - a).
    \]
    
    \begin{proof}
        $f$ continuous hence takes minimum,
        $m$,
        and maximum,
        $M$,
        on $[a, b]$.
        By monotonicity there exists $C$ such that
        \[
        C = \int_{a}^{b}f(x)\,dx / (b - a)
        \]
        with $m \leq C \leq M$.
        By the intermediate value theorem there exists $c \in [a, b]$ such that $f(c) = C$,
        so
        \[
        f(c)(b - a) = C(b - a) = \int_{a}^{b}f(x)\,dx.
        \]
    \end{proof}
\end{theorem}

\begin{theorem}
    $f : [a, b]$ continuous.
    Assume $f(x) \geq 0$ and exists a point $c \in [a, b]$ such that $f(c) > 0$.
    Then
    \[
    \int_{a}^{b}f(t)\,dt > 0.
    \]
    
    \begin{proof}
        Set $C = f(c)$.
        By continuity there exists a $\delta > 0$ such that $f(x) \geq \frac{c}{2}$ for all $x \in (c - \delta, c + \delta)$.
        With this
        \begin{align*}
            \int_{a}^{b}f(t)\,dt &= \int_{a}^{c - \delta}f(t)\,dt + \int_{c - \delta}^{c + \delta}f(t)\,dt + \int_{c + \delta}^{b}f(t)\,dt \\
            &\geq 0 + \frac{C}{2}\cdot 2\cdot \delta + 0 \\
            &= C\delta \\
            &> 0.
        \end{align*}
    \end{proof}
\end{theorem}

\begin{definition}
    For $a \geq b$ set
    \[
    \int_{a}^{b}f(x)\,dx \coloneqq -\int_{b}^{a}f(x)\,dx.
    \]
\end{definition}

\subsection{Fundamental Theorem of Calculus}

\begin{proposition}
    $f$ is regulated on $[a, b]$.
    $F(x) = \int_{a}^{x}f(t)\,dt$ is Lipschitz continuous on $[a, b]$ with $M = \sup_{x \in [a, b]}|f(x)|$.
    In particular,
    $f$ is continuous on $[a, b]$.

    \begin{proof}
        \begin{align*}
            |F(x) - F(y)| &= \left|\int_{a}^{x}f(t)\,dt - \int_{a}^{y}f(t)\,dt\right| \\
            &= \left|\int_{y}^{x}f(t)\,dt\right| \\
            &\leq \left|\int_{y}^{x}|f(t)|\,dt\right| \\
            &\leq \left|\int_{y}^{x}M\,dt\right| \\
            &= M|x - y|
        \end{align*}
        hence Lipschitz.
    \end{proof}
\end{proposition}

\begin{theorem}[Fundamental Theorem of Calculus]
    $f$ continuous on $[a, b]$.
    Then $F(x) = \int_{a}^{x}f(t)\,dt$ is differentiable on $[a, b]$ with
    \[
    F'(x) = f(x).
    \]

    \begin{proof}
        Let $c \in (a, b)$.
        Need to show
        \[
        \lim_{x \to c}\frac{F(x) - F(c)}{x - c}
        \]
        exists and is equal to $f(c)$.
        For $x \neq c$
        \begin{align*}
            \frac{1}{x - c}\left(\int_{a}^{x}f(t)\,dt - \int_{a}^{c}f(t)\,dt\right) &= \frac{1}{x - c}\int_{c}^{x}f(t)\,dt \\
            \intertext{Assume $x > c$
            ($c > x$ goes the same).
            By mean value theorem for integrals}
            &= \frac{1}{x - c}\cdot (x - c)f(\xi_x) \\
            \intertext{with $\xi_x \in (c, x)$}
            &= f(\xi_x) \to f(c)
        \end{align*}
        since $x \to c$ so does $\xi_x \to c$ by continuity.
    \end{proof}
\end{theorem}

\subsection{Limit Theorems}

\begin{theorem}
    $f_n : [a, b] \to \R$ sequence of regulated functions such that $f_n \to f$ uniformly.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item $f$ is regulated.
        
        \item
        \[
        \int_{a}^{b}f(t)\,dt = \liminfty\int_{a}^{b}f_n(t)\,dt.
        \]
    \end{enumerate}

    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
            \item
            $f_n \to f$ uniformly.
            So,
            given $\varepsilon > 0$.
            \[
            |f_n(x) - f(x)| < \varepsilon
            \]
            for all $x \in [a, b]$ and for all $n \geq N$.
            $f_n$ regulated.
            There exists step functions $g_n(x)$ such that
            \[
            |f_n(x) - g_n(x)| < \varepsilon
            \]
            for all $x \in [a, b]$.
            Claim $g_n \to f$ uniformly.
            So $f$ is regulated.
            \begin{align*}
                |f(x) - g_n(x)| &= |f(x) - f_n(x) + f_n(x) - g_n(x)| \\
                &\leq |f(x) - f_n(x)| + |f_n(x) - g_n(x)| \\
                &< \varepsilon + \varepsilon = 2\varepsilon
            \end{align*}
            for all $x \in [a, b]$ and for all $n \geq N$.

            \item
            \begin{align*}
                \left|\int_{a}^{b}f(t)\,dt - \int_{a}^{b}f_n(t)\,dt\right| &= \left|\int_{a}^{b}|f(t) - f_n(t)|\,dt\right| \\
                &\leq \int_{a}^{b}|f(t) - f_n(t)|\,dt \\
                &\leq \int_{a}^{b}\varepsilon\,dt &(n \geq N) \\
                &= (b - a)\varepsilon.
            \end{align*}
        \end{enumerate}
    \end{proof}
\end{theorem}

\begin{theorem}
    Let $f_n(x)$ sequence of functions with continuous derivatives $f_n'(x)$.
    Assume $f_n \to f$ pointwise,
    $f_n' \to g$ uniformly for some function $g(x)$.

    Then $f(x)$ is differentiable and $f'(x) = g(x) = \liminfty f_n'(x)$.

    \begin{proof}
        Have
        \[
        f_n(x) = f_n(a) + \int_{a}^{x}f_n'(t)\,dt,
        \]
        by the fundamental theorem of calculus,
        as $n \to \infty$
        \[
        f(x) = f(a) + \liminfty\int_{a}^{x}g(t)\,dt\footnotemark
        \]
        \footnotetext{By the previous theorem.}
        by the fundamental theorem of calculus $\int_{a}^{x}g(t)\,dt$ is differentiable with derivative $g(x)$.
    \end{proof}
\end{theorem}

\subsection{Improper Integrals}

\begin{theorem}[Integral test]
    $f : (N, \infty) \to (0, \infty)$ continuous and decreasing.
    Set $a_k = f(k)$.
    \[
    \infsum[k = N]a_k\text{ converges} \iff \int_{N}^{\infty}f(t)\,dt\text{ converges}.
    \]

    \begin{proof}
        \[
        \sum_{k = N + 1}^{x}a_k \leq \int_{N}^{x}f(t)\,dt \leq \sum_{k = N}^{x - 1}a_k \leq \int_{N + 1}^{x + 1}f(t)\,dt
        \]
        converging by comparison and squeeze.
    \end{proof}
\end{theorem}

\begin{corollary}
    \[
    \infsum[n = 1]n ^ {\alpha}
    \]
    converges if and only if $\alpha < -1$.
\end{corollary}

\end{document}
